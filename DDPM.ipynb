{"cells":[{"cell_type":"markdown","source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/khetansarvesh/CV/blob/main/DDPM.ipynb)"],"metadata":{"id":"cAlu_3SWMg7T"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"eqV5stpLOf5_"},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torchvision\n","from matplotlib import pyplot as plt\n","from tqdm import tqdm\n","from torch import optim\n","from torch.utils.tensorboard import SummaryWriter\n","from PIL import Image\n","from torch.utils.data import DataLoader\n","import torch.nn.functional as F\n","from torch.optim import Adam\n","from torchvision.utils import make_grid"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tn_tgB-y3-cD"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","metadata":{"id":"KK3CuUmjPe1k"},"source":["# Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tm5iBa-WDMjX"},"outputs":[],"source":["from torchvision import datasets, transforms\n","from torch.utils.data.dataset import Dataset\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1465,"status":"ok","timestamp":1704164525540,"user":{"displayName":"Sarvesh Khetan","userId":"05805229168588767785"},"user_tz":-330},"id":"5Gq3GuRbDNDG","outputId":"e16af869-5f88-4b87-dfdd-b2d3f4f0e157"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 81147722.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 28881/28881 [00:00<00:00, 133703856.32it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 42022478.35it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4542/4542 [00:00<00:00, 22359775.55it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}],"source":["transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) # Define the transformation to normalize the data between 1 and -1 (mean = 0.5 and variance = 0.5 will transform to values between 1 and -1)\n","# transforms = torchvision.transforms.Compose([\n","#       torchvision.transforms.Resize(80),  # args.image_size + 1/4 *args.image_size\n","#       torchvision.transforms.RandomResizedCrop(64, scale=(0.8, 1.0)),\n","#       torchvision.transforms.ToTensor(),\n","#       torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","#   ])\n","mnist = datasets.MNIST(root='./data', train=True, transform=transform, download=True) # downloading the MNIST train dataset and then applying some transformations\n","mnist_loader = DataLoader(dataset=mnist, batch_size=64, shuffle=True, num_workers=4) # loading the downloaded dataset"]},{"cell_type":"markdown","metadata":{"id":"9cT3anjwurX9"},"source":["# Modelling"]},{"cell_type":"markdown","metadata":{"id":"6XE9dhdPW6xR"},"source":["## Autoencoder (Unet)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-eu6NQ5jlvWx"},"outputs":[],"source":["'''\n","Input :\n","  time_steps : takes in 1D array of timesteps eg [1,10,500,40,300]\n","  temb_dim : dimension of vector to which each of these timestep needs to be converted to eg 128\n","\n","Output :\n","  t_emd : for each scalar timestep in time_steps 1D array, convert it to a vector of dimension temb_dim\n","'''\n","def get_time_embedding(time_steps, temb_dim):\n","\n","    # factor = 10000^(2i/d_model)\n","    factor = 10000 ** ((torch.arange(start=0, end=temb_dim // 2, dtype=torch.float32, device=time_steps.device) / (temb_dim // 2)))\n","\n","    # pos / factor\n","    t_emb = time_steps[:, None].repeat(1, temb_dim // 2) / factor\n","\n","    # now taking sin and cos of t_emb\n","    return torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eJNhWiyAlwv1"},"outputs":[],"source":["class DownBlock(nn.Module):\n","\n","    def __init__(self, in_channels, out_channels, t_emb_dim, down_sample, num_heads=4):\n","        super().__init__()\n","\n","        self.resnet_conv_first = nn.Sequential( nn.GroupNorm(8, in_channels), nn.SiLU(), nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)) # Resnet block\n","        self.t_emb_layers = nn.Sequential( nn.SiLU(), nn.Linear(t_emb_dim, out_channels) ) # time embedding\n","        self.resnet_conv_second = nn.Sequential( nn.GroupNorm(8, out_channels), nn.SiLU(), nn.Conv2d(out_channels, out_channels,kernel_size=3, stride=1, padding=1) ) # resnet block 2\n","\n","        # Attention block\n","        # self.attention_norms = layer_1 = nn.GroupNorm(8, out_channels)\n","        # self.attentions = nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n","        self.residual_input_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","\n","        # Downsample using 2x2 average pooling\n","        self.down_sample_conv = nn.Conv2d(out_channels, out_channels, kernel_size=4, stride=2, padding=1) if down_sample else nn.Identity()\n","\n","    def forward(self, x, t_emb):\n","        out = x\n","\n","        # Resnet block of Unet\n","        resnet_input = out\n","        out = self.resnet_conv_first(out)\n","        out = out + self.t_emb_layers(t_emb)[:, :, None, None]\n","        out = self.resnet_conv_second(out)\n","        out = out + self.residual_input_conv(resnet_input)\n","\n","        # Attention block of Unet\n","        # batch_size, channels, h, w = out.shape\n","        # in_attn = out.reshape(batch_size, channels, h * w)\n","        # in_attn = self.attention_norms(in_attn)\n","        # in_attn = in_attn.transpose(1, 2)\n","        # out_attn, _ = self.attentions(in_attn, in_attn, in_attn)\n","        # out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n","        # out = out + out_attn\n","\n","        out = self.down_sample_conv(out)\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ymee4kYJrbfm"},"outputs":[],"source":["class MidBlock(nn.Module):\n","\n","    def __init__(self, in_channels, out_channels, t_emb_dim, num_heads=4):\n","        super().__init__()\n","\n","        # Resnet Block 1\n","        layer_1 = nn.Sequential(nn.GroupNorm(8, in_channels), nn.SiLU(), nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1,padding=1))\n","        layer_2 = nn.Sequential(nn.GroupNorm(8, out_channels), nn.SiLU(), nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1,padding=1))\n","        self.resnet_conv_first = nn.ModuleList( [layer_1, layer_2] )\n","\n","        # time embedding\n","        layer = nn.Sequential(nn.SiLU(),nn.Linear(t_emb_dim, out_channels))\n","        self.t_emb_layers = nn.ModuleList([ layer, layer])\n","\n","        # Resnet Block 2\n","        layer = nn.Sequential(nn.GroupNorm(8, out_channels), nn.SiLU(), nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1))\n","        self.resnet_conv_second = nn.ModuleList([ layer, layer])\n","\n","        # Attention block\n","        # self.attention_norms = nn.GroupNorm(8, out_channels)\n","        # self.attentions = nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n","\n","        # Resnet block\n","        layer_1 = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","        layer_2 = nn.Conv2d(out_channels, out_channels, kernel_size=1)\n","        self.residual_input_conv = nn.ModuleList([layer_1, layer_2])\n","\n","    def forward(self, x, t_emb):\n","        out = x\n","\n","        # resnet block 1\n","        resnet_input = out\n","        out = self.resnet_conv_first[0](out)\n","        out = out + self.t_emb_layers[0](t_emb)[:, :, None, None]\n","        out = self.resnet_conv_second[0](out)\n","        out = out + self.residual_input_conv[0](resnet_input)\n","\n","        # Attention Block\n","        # batch_size, channels, h, w = out.shape\n","        # in_attn = out.reshape(batch_size, channels, h * w)\n","        # in_attn = self.attention_norms(in_attn)\n","        # in_attn = in_attn.transpose(1, 2)\n","        # out_attn, _ = self.attentions(in_attn, in_attn, in_attn)\n","        # out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n","        # out = out + out_attn\n","\n","        # Resnet Block 2\n","        resnet_input = out\n","        out = self.resnet_conv_first[1](out)\n","        out = out + self.t_emb_layers[1](t_emb)[:, :, None, None]\n","        out = self.resnet_conv_second[1](out)\n","        out = out + self.residual_input_conv[1](resnet_input)\n","\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x1qGGTc4rkPS"},"outputs":[],"source":["class UpBlock(nn.Module):\n","\n","    def __init__(self, in_channels, out_channels, t_emb_dim, up_sample, num_heads=4, num_layers=1):\n","        super().__init__()\n","\n","        # Upsample\n","        self.up_sample_conv = nn.ConvTranspose2d(in_channels // 2, in_channels // 2,kernel_size=4, stride=2, padding=1) if up_sample else nn.Identity()\n","\n","        # Resnet block\n","        self.resnet_conv_first = nn.Sequential( nn.GroupNorm(8, in_channels), nn.SiLU(), nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1,padding=1) )\n","        # time embedding\n","        self.t_emb_layers = nn.Sequential( nn.SiLU(), nn.Linear(t_emb_dim, out_channels))\n","        # resnet block 2\n","        self.resnet_conv_second = nn.Sequential(nn.GroupNorm(8, out_channels), nn.SiLU(), nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1))\n","\n","        # Attention Block\n","        # self.attention_norms = nn.GroupNorm(8, out_channels)\n","        # self.attentions = nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n","\n","        # Concatenate Down block output\n","        self.residual_input_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","\n","\n","\n","    def forward(self, x, out_down, t_emb):\n","        x = self.up_sample_conv(x)\n","        x = torch.cat([x, out_down], dim=1)\n","\n","        out = x\n","\n","        resnet_input = out\n","        out = self.resnet_conv_first(out)\n","        out = out + self.t_emb_layers(t_emb)[:, :, None, None]\n","        out = self.resnet_conv_second(out)\n","        out = out + self.residual_input_conv(resnet_input)\n","\n","        # batch_size, channels, h, w = out.shape\n","        # in_attn = out.reshape(batch_size, channels, h * w)\n","        # in_attn = self.attention_norms(in_attn)\n","        # in_attn = in_attn.transpose(1, 2)\n","        # out_attn, _ = self.attentions(in_attn, in_attn, in_attn)\n","        # out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n","        # out = out + out_attn\n","\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2HKZ9V0qltJJ"},"outputs":[],"source":["class Unet(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        im_channels = 1\n","        self.t_emb_dim = 128\n","\n","        # Initial projection from sinusoidal time embedding\n","        self.t_proj = nn.Sequential( nn.Linear(self.t_emb_dim, self.t_emb_dim), nn.SiLU(), nn.Linear(self.t_emb_dim, self.t_emb_dim))\n","\n","        # 1 layer of conv2d, you can add more conv2d layers if you want to and between these conv layer add group norm followed by activation function like GELU()\n","        self.conv_in = nn.Conv2d(im_channels, 32, kernel_size=3, padding=(1, 1))\n","\n","        # down block\n","        down_layer_1 = DownBlock(32, 128, self.t_emb_dim, True)\n","        # down_layer_2 = DownBlock(64, 128, self.t_emb_dim, True)\n","        down_layer_2 = DownBlock(128, 256, self.t_emb_dim, False)\n","        self.downs = nn.ModuleList([down_layer_1, down_layer_2])\n","\n","        # mid block\n","        mid_layer_1 = MidBlock(256, 256, self.t_emb_dim)\n","        mid_layer_2 = MidBlock(256, 128, self.t_emb_dim)\n","        self.mids = nn.ModuleList([mid_layer_1, mid_layer_2])\n","\n","        # up block\n","        up_layer_1 = UpBlock(128 * 2, 32, self.t_emb_dim, False)\n","        # up_layer_2 = UpBlock(64 * 2, 32, self.t_emb_dim, True)\n","        up_layer_2 = UpBlock(32 * 2, 16, self.t_emb_dim, True)\n","        self.ups = nn.ModuleList([up_layer_1, up_layer_2])\n","\n","        # groupNorm\n","        self.norm_out = nn.GroupNorm(8, 16)\n","\n","        # conv2d\n","        self.conv_out = nn.Conv2d(16, im_channels, kernel_size=3, padding=1)\n","\n","    def forward(self, x, t):\n","        # Shapes assuming downblocks are [C1, C2, C3, C4]\n","        # Shapes assuming midblocks are [C4, C4, C3]\n","        # Shapes assuming downsamples are [True, True, False]\n","        # B x C x H x W\n","        out = self.conv_in(x)\n","        # B x C1 x H x W\n","\n","        # t_emb -> B x t_emb_dim\n","        t_emb = get_time_embedding(torch.as_tensor(t).long(), self.t_emb_dim)\n","        t_emb = self.t_proj(t_emb)\n","\n","\n","        # keep saving outputs of down block cause we need it as input for upblock\n","        down_outs = []\n","        for down in self.downs:\n","            down_outs.append(out)\n","            out = down(out, t_emb)\n","        # down_outs  [B x C1 x H x W, B x C2 x H/2 x W/2, B x C3 x H/4 x W/4]\n","        # out B x C4 x H/4 x W/4\n","\n","        for mid in self.mids:\n","            out = mid(out, t_emb)\n","        # out B x C3 x H/4 x W/4\n","\n","        for up in self.ups:\n","            down_out = down_outs.pop()\n","            out = up(out, down_out, t_emb)\n","            # out [B x C2 x H/4 x W/4, B x C1 x H/2 x W/2, B x 16 x H x W]\n","        out = self.norm_out(out)\n","        out = nn.SiLU()(out)\n","        out = self.conv_out(out)\n","        # out B x C x H x W\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"lD79DmLuXed3"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cFSG78lRxUry"},"outputs":[],"source":["batch_size = 64\n","num_samples = 100\n","num_grid_rows = 10\n","\n","model = Unet().to(device)\n","model.train()\n","optimizer = Adam(model.parameters(), lr = 0.0001)\n","\n","betas = torch.linspace(0.0001, 0.02, 1000).to(device) # creating a linear beta schedule for all the timestamps\n","alpha_cum_prod = torch.cumprod(1. - betas, dim=0).to(device) # calculating alpha_bar for each timestamp\n","sqrt_alpha_cum_prod = torch.sqrt(alpha_cum_prod).to(device) # calculating sqrt(alpha_bar) for each timestamp\n","sqrt_one_minus_alpha_cum_prod = torch.sqrt(1 - alpha_cum_prod).to(device) # calculating sqrt(1-alpha_bar) for each timestamp"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"jUG0FOoW3HYr","outputId":"bccb4975-a14f-48f3-aa5a-efb1c55d2946"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 938/938 [01:18<00:00, 11.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 938/938 [01:19<00:00, 11.83it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 938/938 [01:19<00:00, 11.82it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 938/938 [01:19<00:00, 11.79it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 938/938 [01:19<00:00, 11.84it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 938/938 [01:19<00:00, 11.84it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 938/938 [01:19<00:00, 11.85it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 938/938 [01:19<00:00, 11.83it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 938/938 [01:19<00:00, 11.85it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 938/938 [01:19<00:00, 11.86it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 938/938 [01:19<00:00, 11.84it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 938/938 [01:19<00:00, 11.82it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 938/938 [01:19<00:00, 11.83it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 938/938 [01:19<00:00, 11.80it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 938/938 [01:19<00:00, 11.77it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 938/938 [01:19<00:00, 11.79it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 938/938 [01:19<00:00, 11.84it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 938/938 [01:19<00:00, 11.85it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 938/938 [01:19<00:00, 11.83it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 938/938 [01:19<00:00, 11.85it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 938/938 [01:19<00:00, 11.85it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 938/938 [01:19<00:00, 11.82it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":[" 70%|███████   | 661/938 [00:56<00:23, 11.86it/s]"]}],"source":["for epoch in range(40): # running for 40 epochs\n","  losses = []\n","  for im,_ in tqdm(mnist_loader):\n","    optimizer.zero_grad()\n","\n","    im = im.float().to(device)\n","    noise = torch.randn_like(im).to(device) # sample random noise\n","    t = torch.randint(low = 0, high = 1000, size = (im.shape[0],)).to(device) # sample a random timestamp for each image in the batch\n","    noisy_im = torch.sqrt(alpha_cum_prod[t])[:, None, None, None].to(device) * im + torch.sqrt(1 - alpha_cum_prod[t])[:, None, None, None].to(device) * noise # add noise to image according to the timestamp\n","    noise_pred = model(noisy_im, t) # predicting the added noise\n","\n","    loss = torch.nn.MSELoss()(noise_pred, noise) # loss fucntion\n","    losses.append(loss.item())\n","    loss.backward() # backpropagating the loss\n","    optimizer.step()\n","  print()"]},{"cell_type":"markdown","metadata":{"id":"DbBa_irqrnkT"},"source":["# Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jJ0IZKEpNFPq"},"outputs":[],"source":["model.eval()\n","batch_size = 64\n","num_epochs = 40\n","num_samples = 100\n","num_grid_rows = 10\n","im_channels = 1\n","im_size = 28"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"epLrrEch5FU6"},"outputs":[],"source":["with torch.no_grad():\n","    # creating a random noise sample based on number of images requested to generate\n","    xt = torch.randn((num_samples, im_channels, im_size, im_size)).to(device)\n","    for i in tqdm(reversed(range(1000))):\n","\n","        t = torch.as_tensor(i)\n","        noise_pred = model(xt, t.unsqueeze(0).to(device)) # Get prediction of noise\n","\n","        # timestep\n","        t = t.to(device)\n","\n","        # calculating Xt-1 using the derived formula\n","        mean = (xt - ((betas[t])*noise_pred)/(sqrt_one_minus_alpha_cum_prod[t])) / torch.sqrt(1. - betas[t])\n","        variance = ( (1-alpha_cum_prod[t-1]) / (1.0 - alpha_cum_prod[t]) ) * betas[t]\n","        sigma = variance ** 0.5 if t != 0 else 0\n","        xt = mean + sigma * torch.randn(xt.shape).to(xt.device)\n","\n","        # Save x0\n","        ims = torch.clamp(xt, -1., 1.).detach().cpu()\n","        ims = 0.5*ims + 0.5\n","        grid = make_grid(ims, nrow= num_grid_rows)\n","\n","        # predicted image\n","        img = torchvision.transforms.ToPILImage()(grid)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["KK3CuUmjPe1k","9cT3anjwurX9","6XE9dhdPW6xR","lD79DmLuXed3","DbBa_irqrnkT"],"provenance":[],"authorship_tag":"ABX9TyOikT6q/aFui2eEHTdSrLdL"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}