{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"exXZVJAPGQ58"},"outputs":[],"source":["import sys\n","import threading\n","import argparse\n","import os\n","from math import ceil\n","from enum import Enum\n","import imageio\n","from tqdm import tqdm\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from dataclasses import dataclass\n","from PIL import Image\n","from typing import List, Tuple\n","from pathlib import Path\n","import random\n","import xml.etree.ElementTree as ET\n","import torch as t\n","from torch import nn\n","from torch.nn import functional as F\n","import torchvision"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nzyj_z0JjAi-"},"outputs":[],"source":["# variables\n","# backbone = \"resnet50\"\n","# cache_images = # Cache images during training (requires ample CPU memory)\")\n","# periodic_eval_samples = 1000 # \"Number of samples to use during evaluation after each epoch\")\n","# plot =  # \"Plots the average precision of each class after evaluation (use with --train or --eval)\")\n","# no_augment = # if no value passed then \"Disable image augmentation (random horizontal flips) during training else enable\n","# exclude_edge_proposals = # Exclude edge proposals generated at anchors spanning image edges from being passed to detector stage\n","# profile_cuda_memory = # \"Profile CUDA memory usage and write output to 'cuda_memory.txt'\")\n","# save_to = # file name || \"Save final trained weights to file\"\n","# load_from =  # Load initial model weights from file\n","# initial_weights = \"IMAGENET1K_V1\"\n","# log_csv = #Log training metrics to CSV file\n","# dump_anchors =\n","# checkpoint_dir = #Save checkpoints after each epoch to the given directory\")\n","\n","\n","# def no_grad(func):\n","#   def wrapper_nograd(*args, **kwargs):\n","#     with t.no_grad():\n","#       return func(*args, **kwargs)\n","#   return wrapper_nograd"]},{"cell_type":"markdown","metadata":{"id":"oPW67aIF3ZDn"},"source":["# Foundation Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lx_H1qw73cbv"},"outputs":[],"source":["class FeatureExtractor(nn.Module):\n","  def __init__(self, resnet):\n","    super().__init__()\n","\n","    # Feature extractor layers\n","    self._feature_extractor = nn.Sequential(\n","      resnet.conv1,     # 0\n","      resnet.bn1,       # 1\n","      resnet.relu,      # 2\n","      resnet.maxpool,   # 3\n","      resnet.layer1,    # 4\n","      resnet.layer2,    # 5\n","      resnet.layer3     # 6\n","    )\n","\n","    # Freeze initial layers\n","    self._freeze(resnet.conv1)\n","    self._freeze(resnet.bn1)\n","    self._freeze(resnet.layer1)\n","\n","    # Ensure that all batchnorm layers are frozen\n","    self._freeze_batchnorm(self._feature_extractor)\n","\n","  # Override nn.Module.train()\n","  def train(self, mode = True):\n","    super().train(mode)\n","\n","    #\n","    # During training, set all frozen blocks to evaluation mode and ensure that\n","    # all the batchnorm layers are also in evaluation mode. This is extremely\n","    # important and neglecting to do this will result in severely degraded\n","    # training performance.\n","    #\n","    if mode:\n","      # Set fixed blocks to be in eval mode\n","      self._feature_extractor.eval()\n","      self._feature_extractor[5].train()\n","      self._feature_extractor[6].train()\n","\n","      # *All* batchnorm layers in eval mode\n","      def set_bn_eval(module):\n","        if type(module) == nn.BatchNorm2d:\n","          module.eval()\n","      self._feature_extractor.apply(set_bn_eval)\n","\n","  def forward(self, image_data):\n","    y = self._feature_extractor(image_data)\n","    return y\n","\n","  @staticmethod\n","  def _freeze(layer):\n","    for name, parameter in layer.named_parameters():\n","      parameter.requires_grad = False\n","\n","  def _freeze_batchnorm(self, block):\n","    for child in block.modules():\n","      if type(child) == nn.BatchNorm2d:\n","        self._freeze(layer = child)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-vrbA1xH3cZI"},"outputs":[],"source":["class PoolToFeatureVector(nn.Module):\n","  def __init__(self, resnet):\n","    super().__init__()\n","    self._layer4 = resnet.layer4\n","    self._freeze_batchnorm(self._layer4)\n","\n","  def train(self, mode = True):\n","    # See comments in FeatureVector.train()\n","    super().train(mode)\n","    if mode:\n","      def set_bn_eval(module):\n","        if type(module) == nn.BatchNorm2d:\n","          module.eval()\n","      self._layer4.apply(set_bn_eval)\n","\n","  def forward(self, rois):\n","    y = self._layer4(rois)  # (N, 1024, 7, 7) -> (N, 2048, 4, 4)\n","\n","    # Average together the last two dimensions to remove them -> (N, 2048).\n","    # It is also possible to max pool, e.g.:\n","    # y = F.adaptive_max_pool2d(y, output_size = 1).squeeze()\n","    # This may even be better (74.96% mAP for ResNet50 vs. 73.2% using the\n","    # current method).\n","    y = y.mean(-1).mean(-1) # use mean to remove last two dimensions -> (N, 2048)\n","    return y\n","\n","  @staticmethod\n","  def _freeze(layer):\n","    for name, parameter in layer.named_parameters():\n","      parameter.requires_grad = False\n","\n","  def _freeze_batchnorm(self, block):\n","    for child in block.modules():\n","      if type(child) == nn.BatchNorm2d:\n","        self._freeze(layer = child)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DO4FYvN6HaNF"},"outputs":[],"source":["# Backbone base class, for wrapping backbone models that provide feature\n","# extraction and pooled feature reduction layers from the classifier\n","# stages.\n","#\n","# The backbone in Faster R-CNN is used in two places:\n","#\n","#   1. In Stage 1 as the feature extractor. Given an input image, a feature map\n","#      is produced that is then passed into both the RPN and detector stages.\n","#   2. In Stage 3, the detector, proposal regions are pooled and cropped from\n","#      the feature map (to produce RoIs) and fed into the detector layers,\n","#      which perform classification and bounding box regression. Each RoI must\n","#      first be converted into a linear feature vector. With VGG-16, for\n","#      example, the fully-connected layers following the convolutional layers\n","#      and preceding the classifier layer, are used to do this.\n","\n","class Backbone:\n","  \"\"\"\n","  Backbone base class. When overriding, ensure all members and methods are\n","  defined.\n","  \"\"\"\n","  def __init__(self):\n","    # Required properties\n","    self.feature_map_channels = 0     # feature map channels\n","    self.feature_pixels = 0           # feature size in pixels, N: each feature map cell corresponds to an NxN area on original image\n","    self.feature_vector_size = 0      # length of linear feature vector after pooling and just before being passed to detector heads\n","    self.image_preprocessing_params = PreprocessingParams(channel_order = ChannelOrder.BGR, scaling = 1.0, means = [ 103.939, 116.779, 123.680 ], stds = [ 1, 1, 1 ])\n","\n","    # Required members\n","    self.feature_extractor = None       # nn.Module converting input image (batch_size, channels, width, height) -> (batch_size, feature_map_channels, W, H)\n","    self.pool_to_feature_vector = None  # nn.Module converting RoIs (N, feature_map_channels, 7, 7) -> (N, feature_vector_size)\n","\n","  def compute_feature_map_shape(self, image_shape):\n","    \"\"\"\n","    Computes the shape of the feature extractor output given an input image\n","    shape. This is used primarily for anchor generation and depends entirely on\n","    the architecture of the backbone.\n","\n","    Parameters\n","    ----------\n","    image_shape : Tuple[int, int, int]\n","      Shape of the input image, (channels, height, width). Only the last two\n","      dimensions are relevant, allowing image_shape to be either the shape\n","      of a single image or the entire batch.\n","\n","    Returns\n","    -------\n","    Tuple[int, int, int]\n","      Shape of the feature map produced by the feature extractor,\n","      (feature_map_channels, feature_map_height, feature_map_width).\n","    \"\"\"\n","    return image_shape[-3:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4E0QtgL13cWk"},"outputs":[],"source":["class ResNetBackbone(Backbone):\n","  def __init__(self):\n","    super().__init__()\n","\n","    # Backbone properties. Image preprocessing parameters are common to all\n","    # Torchvision ResNet models and are described in the documentation, e.g.,\n","    # https://pytorch.org/vision/stable/models/generated/torchvision.models.resnet50.html#torchvision.models.resnet50\n","    self.feature_map_channels = 1024  # feature extractor output channels\n","    self.feature_pixels = 16          # ResNet feature maps are 1/16th of the original image size, similar to VGG-16 feature extractor\n","    self.feature_vector_size = 2048   # linear feature vector size after pooling\n","    self.image_preprocessing_params = PreprocessingParams(channel_order = ChannelOrder.RGB, scaling = 1.0 / 255.0, means = [ 0.485, 0.456, 0.406 ], stds = [ 0.229, 0.224, 0.225 ])\n","\n","    # Loading IMAGENET1K_V1 pre-trained weights for Torchvision resnet50 backbone\n","    resnet = torchvision.models.resnet50(weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V1)\n","\n","    # Feature extractor: given image data of shape (batch_size, channels, height, width), produces a feature map of shape (batch_size, 1024, ceil(height/16), ceil(width/16))\n","    self.feature_extractor = FeatureExtractor(resnet = resnet)\n","\n","    # Conversion of pooled features to head input\n","    self.pool_to_feature_vector = PoolToFeatureVector(resnet = resnet)\n","\n","  def compute_feature_map_shape(self, image_shape):\n","    \"\"\"\n","    Computes feature map shape given input image shape. Unlike VGG-16, ResNet\n","    convolutional layers use padding and the resultant dimensions are therefore\n","    not simply an integral division by 16. The calculation here works well\n","    enough but it is not guaranteed that the simple conversion of feature map\n","    coordinates to input image pixel coordinates in anchors.py is absolutely\n","    correct.\n","\n","    Parameters\n","    ----------\n","    image_shape : Tuple[int, int, int]\n","      Shape of the input image, (channels, height, width). Only the last two\n","      dimensions are relevant, allowing image_shape to be either the shape\n","      of a single image or the entire batch.\n","\n","    Returns\n","    -------\n","    Tuple[int, int, int]\n","      Shape of the feature map produced by the feature extractor,\n","      (feature_map_channels, feature_map_height, feature_map_width).\n","    \"\"\"\n","    image_width = image_shape[-1]\n","    image_height = image_shape[-2]\n","    return (self.feature_map_channels, ceil(image_height / self.feature_pixels), ceil(image_width / self.feature_pixels))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JZTFq3ED3cNt","outputId":"2b4a85df-9d55-4809-b344-1468c153a988"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","100%|██████████| 97.8M/97.8M [00:00<00:00, 167MB/s]\n"]}],"source":["backbone = ResNetBackbone()"]},{"cell_type":"markdown","metadata":{"id":"tsv-kIZ5HGCE"},"source":["# Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"id":"2L-ZJsjzHYjZ","outputId":"3062fc91-d731-4903-f625-43cbadaac2a1"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-03-13 15:18:41--  http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n","Resolving host.robots.ox.ac.uk (host.robots.ox.ac.uk)... 129.67.94.152\n","Connecting to host.robots.ox.ac.uk (host.robots.ox.ac.uk)|129.67.94.152|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 460032000 (439M) [application/x-tar]\n","Saving to: ‘VOCtrainval_06-Nov-2007.tar’\n","\n","VOCtrainval_06-Nov- 100%[===================>] 438.72M  15.4MB/s    in 41s     \n","\n","2024-03-13 15:19:23 (10.7 MB/s) - ‘VOCtrainval_06-Nov-2007.tar’ saved [460032000/460032000]\n","\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'test dataset'"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["'''train+validation dataset'''\n","!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar # downloading the VOC2007 tar file\n","!tar -xf VOCtrainval_06-Nov-2007.tar # extracting the above tar file\n","\n","'''test dataset'''\n","# !wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar # downloading the VOC2007 tar file\n","# !tar -xf VOCtest_06-Nov-2007.tar # extracting the above tar file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5D8JOfBKuXWa"},"outputs":[],"source":["@dataclass\n","class Box:\n","  class_index: int\n","  class_name: str\n","  corners: np.ndarray\n","\n","  def __repr__(self):\n","    return \"[class=%s (%f,%f,%f,%f)]\" % (self.class_name, self.corners[0], self.corners[1], self.corners[2], self.corners[3])\n","\n","  def __str__(self):\n","    return repr(self)\n","\n","@dataclass\n","class TrainingSample:\n","  anchor_map:                 np.ndarray                # shape (feature_map_height,feature_map_width,num_anchors*4), with each anchor as [center_y,center_x,height,width]\n","  anchor_valid_map:           np.ndarray                # shape (feature_map_height,feature_map_width,num_anchors), indicating which anchors are valid (do not cross image boundaries)\n","  gt_rpn_map:                 np.ndarray                # TODO: describe me\n","  gt_rpn_object_indices:      List[Tuple[int,int,int]]  # list of (y,x,k) coordinates of anchors in gt_rpn_map that are labeled as object\n","  gt_rpn_background_indices:  List[Tuple[int,int,int]]  # list of (y,x,k) coordinates of background anchors\n","  gt_boxes:                   List[Box]                 # list of ground-truth boxes, scaled\n","  image_data:                 np.ndarray                # shape (3,height,width), pre-processed and scaled to size expected by model\n","  image:                      Image                     # PIL image data (for debug rendering), scaled\n","  filepath:                   str                       # file path of image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j0JLH1wBqePj"},"outputs":[],"source":["class Dataset:\n","  \"\"\"\n","  A VOC dataset iterator for a particular split (train, val, etc.)\n","  \"\"\"\n","\n","  num_classes = 21\n","  class_index_to_name = {\n","    0:  \"background\",\n","    1:  \"aeroplane\",\n","    2:  \"bicycle\",\n","    3:  \"bird\",\n","    4:  \"boat\",\n","    5:  \"bottle\",\n","    6:  \"bus\",\n","    7:  \"car\",\n","    8:  \"cat\",\n","    9:  \"chair\",\n","    10: \"cow\",\n","    11: \"diningtable\",\n","    12: \"dog\",\n","    13: \"horse\",\n","    14: \"motorbike\",\n","    15: \"person\",\n","    16: \"pottedplant\",\n","    17: \"sheep\",\n","    18: \"sofa\",\n","    19: \"train\",\n","    20: \"tvmonitor\"\n","  }\n","\n","  def __init__(self, split, image_preprocessing_params, compute_feature_map_shape_fn, feature_pixels = 16, dir = \"VOCdevkit/VOC2007\", augment = True, shuffle = True, allow_difficult = False, cache = True):\n","    \"\"\"\n","    Parameters\n","    ----------\n","    split : str\n","      Dataset split to load: train, val, or trainval.\n","    image_preprocessing_params : dataset.image.PreprocessingParams\n","      Image preprocessing parameters to apply when loading images.\n","    compute_feature_map_shape_fn : Callable[Tuple[int, int, int], Tuple[int, int, int]]\n","      Function to compute feature map shape, (channels, height, width), from\n","      input image shape, (channels, height, width).\n","    feature_pixels : int\n","      Size of each cell in the Faster R-CNN feature map in image pixels. This\n","      is the separation distance between anchors.\n","    dir : str\n","      Root directory of dataset.\n","    augment : bool\n","      Whether to randomly augment (horizontally flip) images during iteration\n","      with 50% probability.\n","    shuffle : bool\n","      Whether to shuffle the dataset each time it is iterated.\n","    allow_difficult : bool\n","      Whether to include ground truth boxes that are marked as \"difficult\".\n","    cache : bool\n","      Whether to training samples in memory after first being generated.\n","    \"\"\"\n","    if not os.path.exists(dir):\n","      raise FileNotFoundError(\"Dataset directory does not exist: %s\" % dir)\n","    self.split = split\n","    self._dir = dir\n","    self.class_index_to_name = self._get_classes()\n","    self.class_name_to_index = { class_name: class_index for (class_index, class_name) in self.class_index_to_name.items() }\n","    self.num_classes = len(self.class_index_to_name)\n","    assert self.num_classes == Dataset.num_classes, \"Dataset does not have the expected number of classes (found %d but expected %d)\" % (self.num_classes, Dataset.num_classes)\n","    assert self.class_index_to_name == Dataset.class_index_to_name, \"Dataset does not have the expected class mapping\"\n","    self._filepaths = self._get_filepaths()\n","    self.num_samples = len(self._filepaths)\n","    self._gt_boxes_by_filepath = self._get_ground_truth_boxes(filepaths = self._filepaths, allow_difficult = allow_difficult)\n","    self._i = 0\n","    self._iterable_filepaths = self._filepaths.copy()\n","    self._image_preprocessing_params = image_preprocessing_params\n","    self._compute_feature_map_shape_fn = compute_feature_map_shape_fn\n","    self._feature_pixels = feature_pixels\n","    self._augment = augment\n","    self._shuffle = shuffle\n","    self._cache = cache\n","    self._unaugmented_cached_sample_by_filepath = {}\n","    self._augmented_cached_sample_by_filepath = {}\n","\n","  def __iter__(self):\n","    self._i = 0\n","    if self._shuffle:\n","      random.shuffle(self._iterable_filepaths)\n","    return self\n","\n","  def __next__(self):\n","    if self._i >= len(self._iterable_filepaths):\n","      raise StopIteration\n","\n","    # Next file to load\n","    filepath = self._iterable_filepaths[self._i]\n","    self._i += 1\n","\n","    # Augment?\n","    flip = random.randint(0, 1) != 0 if self._augment else 0\n","    cached_sample_by_filepath = self._augmented_cached_sample_by_filepath if flip else self._unaugmented_cached_sample_by_filepath\n","\n","    # Load and, if caching, write back to cache\n","    if filepath in cached_sample_by_filepath:\n","      sample = cached_sample_by_filepath[filepath]\n","    else:\n","      sample = self._generate_training_sample(filepath = filepath, flip = flip)\n","    if self._cache:\n","      cached_sample_by_filepath[filepath] = sample\n","\n","    # Return the sample\n","    return sample\n","\n","  def _generate_training_sample(self, filepath, flip):\n","    # Load and preprocess the image\n","    scaled_image_data, scaled_image, scale_factor, original_shape = load_image(url = filepath, preprocessing = self._image_preprocessing_params, min_dimension_pixels = 600, horizontal_flip = flip)\n","    _, original_height, original_width = original_shape\n","\n","    # Scale ground truth boxes to new image size\n","    scaled_gt_boxes = []\n","    for box in self._gt_boxes_by_filepath[filepath]:\n","      if flip:\n","        corners = np.array([\n","          box.corners[0],\n","          original_width - 1 - box.corners[3],\n","          box.corners[2],\n","          original_width - 1 - box.corners[1]\n","        ])\n","      else:\n","        corners = box.corners\n","      scaled_box = Box(\n","        class_index = box.class_index,\n","        class_name = box.class_name,\n","        corners = corners * scale_factor\n","      )\n","      scaled_gt_boxes.append(scaled_box)\n","\n","    # Generate anchor maps and RPN truth map\n","    anchor_map, anchor_valid_map = generate_anchor_maps(image_shape = scaled_image_data.shape, feature_map_shape = self._compute_feature_map_shape_fn(scaled_image_data.shape), feature_pixels = self._feature_pixels)\n","    gt_rpn_map, gt_rpn_object_indices, gt_rpn_background_indices = generate_rpn_map(anchor_map = anchor_map, anchor_valid_map = anchor_valid_map, gt_boxes = scaled_gt_boxes)\n","\n","    # Return sample\n","    return TrainingSample(\n","      anchor_map = anchor_map,\n","      anchor_valid_map = anchor_valid_map,\n","      gt_rpn_map = gt_rpn_map,\n","      gt_rpn_object_indices = gt_rpn_object_indices,\n","      gt_rpn_background_indices = gt_rpn_background_indices,\n","      gt_boxes = scaled_gt_boxes,\n","      image_data = scaled_image_data,\n","      image = scaled_image,\n","      filepath = filepath\n","    )\n","\n","  def _get_classes(self):\n","    imageset_dir = os.path.join(self._dir, \"ImageSets\", \"Main\")\n","    classes = set([ os.path.basename(path).split(\"_\")[0] for path in Path(imageset_dir).glob(\"*_\" + self.split + \".txt\") ])\n","    assert len(classes) > 0, \"No classes found in ImageSets/Main for '%s' split\" % self.split\n","    class_index_to_name = { (1 + v[0]): v[1] for v in enumerate(sorted(classes)) }\n","    class_index_to_name[0] = \"background\"\n","    return class_index_to_name\n","\n","  def _get_filepaths(self):\n","    image_list_file = os.path.join(self._dir, \"ImageSets\", \"Main\", self.split + \".txt\")\n","    with open(image_list_file) as fp:\n","      basenames = [ line.strip() for line in fp.readlines() ] # strip newlines\n","    image_paths = [ os.path.join(self._dir, \"JPEGImages\", basename) + \".jpg\" for basename in basenames ]\n","    return image_paths\n","\n","    \"\"\"\n","    # Debug: 60 car training images. Handy for quick iteration and testing.\n","    image_paths = [\n","      \"2008_000028\",\n","      \"2008_000074\",\n","      \"2008_000085\",\n","      \"2008_000105\",\n","      \"2008_000109\",\n","      \"2008_000143\",\n","      \"2008_000176\",\n","      \"2008_000185\",\n","      \"2008_000187\",\n","      \"2008_000189\",\n","      \"2008_000193\",\n","      \"2008_000199\",\n","      \"2008_000226\",\n","      \"2008_000237\",\n","      \"2008_000252\",\n","      \"2008_000260\",\n","      \"2008_000315\",\n","      \"2008_000346\",\n","      \"2008_000356\",\n","      \"2008_000399\",\n","      \"2008_000488\",\n","      \"2008_000531\",\n","      \"2008_000563\",\n","      \"2008_000583\",\n","      \"2008_000595\",\n","      \"2008_000613\",\n","      \"2008_000619\",\n","      \"2008_000719\",\n","      \"2008_000833\",\n","      \"2008_000944\",\n","      \"2008_000953\",\n","      \"2008_000959\",\n","      \"2008_000979\",\n","      \"2008_001018\",\n","      \"2008_001039\",\n","      \"2008_001042\",\n","      \"2008_001104\",\n","      \"2008_001169\",\n","      \"2008_001196\",\n","      \"2008_001208\",\n","      \"2008_001274\",\n","      \"2008_001329\",\n","      \"2008_001359\",\n","      \"2008_001375\",\n","      \"2008_001440\",\n","      \"2008_001446\",\n","      \"2008_001500\",\n","      \"2008_001533\",\n","      \"2008_001541\",\n","      \"2008_001631\",\n","      \"2008_001632\",\n","      \"2008_001716\",\n","      \"2008_001746\",\n","      \"2008_001860\",\n","      \"2008_001941\",\n","      \"2008_002062\",\n","      \"2008_002118\",\n","      \"2008_002197\",\n","      \"2008_002202\",\n","      \"2011_003247\"\n","    ]\n","    return [ os.path.join(self._dir, \"JPEGImages\", path) + \".jpg\" for path in image_paths ]\n","    \"\"\"\n","\n","  def _get_ground_truth_boxes(self, filepaths, allow_difficult):\n","    gt_boxes_by_filepath = {}\n","    for filepath in filepaths:\n","      basename = os.path.splitext(os.path.basename(filepath))[0]\n","      annotation_file = os.path.join(self._dir, \"Annotations\", basename) + \".xml\"\n","      tree = ET.parse(annotation_file)\n","      root = tree.getroot()\n","      assert tree != None, \"Failed to parse %s\" % annotation_file\n","      assert len(root.findall(\"size\")) == 1\n","      size = root.find(\"size\")\n","      assert len(size.findall(\"depth\")) == 1\n","      depth = int(size.find(\"depth\").text)\n","      assert depth == 3\n","      boxes = []\n","      for obj in root.findall(\"object\"):\n","        assert len(obj.findall(\"name\")) == 1\n","        assert len(obj.findall(\"bndbox\")) == 1\n","        assert len(obj.findall(\"difficult\")) == 1\n","        is_difficult = int(obj.find(\"difficult\").text) != 0\n","        if is_difficult and not allow_difficult:\n","          continue  # ignore difficult examples unless asked to include them\n","        class_name = obj.find(\"name\").text\n","        bndbox = obj.find(\"bndbox\")\n","        assert len(bndbox.findall(\"xmin\")) == 1\n","        assert len(bndbox.findall(\"ymin\")) == 1\n","        assert len(bndbox.findall(\"xmax\")) == 1\n","        assert len(bndbox.findall(\"ymax\")) == 1\n","        x_min = int(bndbox.find(\"xmin\").text) - 1  # convert to 0-based pixel coordinates\n","        y_min = int(bndbox.find(\"ymin\").text) - 1\n","        x_max = int(bndbox.find(\"xmax\").text) - 1\n","        y_max = int(bndbox.find(\"ymax\").text) - 1\n","        corners = np.array([ y_min, x_min, y_max, x_max ]).astype(np.float32)\n","        box = Box(class_index = self.class_name_to_index[class_name], class_name = class_name, corners = corners)\n","        boxes.append(box)\n","      assert len(boxes) > 0\n","      gt_boxes_by_filepath[filepath] = boxes\n","    return gt_boxes_by_filepath\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jqmNL3yFD_BD"},"outputs":[],"source":["train_split = \"trainval\" #\"Dataset split to use for training\")\n","eval_split = \"test\" #\"Dataset split to use for evaluation\")\n","dataset_dir = \"VOCdevkit/VOC2007\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"43TgDKbPNSnj"},"outputs":[],"source":["training_data = Dataset(\n","  dir = dataset_dir,\n","  split = train_split,\n","  image_preprocessing_params = backbone.image_preprocessing_params,\n","  compute_feature_map_shape_fn = backbone.compute_feature_map_shape,\n","  feature_pixels = backbone.feature_pixels,\n","  augment = False,\n","  shuffle = True,\n","  cache = False\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aUrvI8QcjJm5","outputId":"ad73112f-e7c9-4724-f87c-02635ee2999e"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-3-14dcf3ba3bf1>:72: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n","  data = imageio.imread(url, pilmode = \"RGB\")\n","<ipython-input-3-14dcf3ba3bf1>:72: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n","  data = imageio.imread(url, pilmode = \"RGB\")\n"]}],"source":["# visualizing anchors and ground truth boxes for the first image\n","c = 0\n","for sample in iter(training_data):\n","  if c == 0:\n","    output_path = os.path.join(\"/content/anchors_temp.png\")\n","    show_anchors(\n","      output_path = output_path,\n","      image = sample.image,\n","      anchor_map = sample.anchor_map,\n","      anchor_valid_map = sample.anchor_valid_map,\n","      gt_rpn_map = sample.gt_rpn_map,\n","      gt_boxes = sample.gt_boxes,\n","      display = True\n","    )\n","  else:\n","    break\n","\n","  c = c+1"]},{"cell_type":"markdown","metadata":{"id":"rZgLZzZyPr9o"},"source":["# Modelling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JmWz2Rw8eCbZ"},"outputs":[],"source":["# PyTorch implementation of the RPN (region proposal network) stage of\n","# Faster R-CNN. Given a feature map (i.e., the output of the VGG-16\n","# convolutional layers), generates objectness scores for each anchor box, and\n","# boxes in the form of modifications to anchor center points and dimensions.\n","#\n","# Unlike the original Faster R-CNN implementation (and many subsequent re-\n","# implementations), which used two outputs per anchor (object and background)\n","# and a softmax activation, this implementation uses only a single output and\n","# sigmoid activation, which is simpler but equivalent. A value of < 0.5 is\n","# background and >= 0.5 is an object.\n","#\n","# The RPN class and box regression losses are defined here.\n","#\n","\n","import numpy as np\n","import torch as t\n","from torch import nn\n","from torch.nn import functional as F\n","from torchvision.ops import nms\n","\n","class RegionProposalNetwork(nn.Module):\n","  def __init__(self, feature_map_channels, allow_edge_proposals = False):\n","    super().__init__()\n","\n","    # Constants\n","    self._allow_edge_proposals = allow_edge_proposals\n","\n","    # Layers\n","    num_anchors = 9\n","    channels = feature_map_channels\n","    self._rpn_conv1 = nn.Conv2d(in_channels = channels, out_channels = channels, kernel_size = (3, 3), stride = 1, padding = \"same\")\n","    self._rpn_class = nn.Conv2d(in_channels = channels, out_channels = num_anchors, kernel_size = (1, 1), stride = 1, padding = \"same\")\n","    self._rpn_boxes = nn.Conv2d(in_channels = channels, out_channels = num_anchors * 4, kernel_size = (1, 1), stride = 1, padding = \"same\")\n","\n","    # Initialize weights\n","    self._rpn_conv1.weight.data.normal_(mean = 0.0, std = 0.01)\n","    self._rpn_conv1.bias.data.zero_()\n","    self._rpn_class.weight.data.normal_(mean = 0.0, std = 0.01)\n","    self._rpn_class.bias.data.zero_()\n","    self._rpn_boxes.weight.data.normal_(mean = 0.0, std = 0.01)\n","    self._rpn_boxes.bias.data.zero_()\n","\n","  def forward(self, feature_map, image_shape, anchor_map, anchor_valid_map, max_proposals_pre_nms, max_proposals_post_nms):\n","    \"\"\"\n","    Predict objectness scores and regress region-of-interest box proposals on\n","    an input feature map.\n","\n","    Parameters\n","    ----------\n","    feature_map : torch.Tensor\n","      Feature map of shape (batch_size, feature_map_channels, height, width).\n","    image_shape : Tuple[int, int, int]\n","      Shapes of each image in pixels: (num_channels, height, width).\n","    anchor_map : np.ndarray\n","      Map of anchors, shaped (height, width, num_anchors * 4). The last\n","      dimension contains the anchor boxes specified as a 4-tuple of\n","      (center_y, center_x, height, width), repeated for all anchors at that\n","      coordinate of the feature map.\n","    anchor_valid_map : np.ndarray\n","      Map indicating which anchors are valid (do not intersect image bounds),\n","      shaped (height, width, num_anchors).\n","    max_proposals_pre_nms : int\n","      How many of the best proposals (sorted by objectness score) to extract\n","      before applying non-maximum suppression.\n","    max_proposals_post_nms : int\n","      How many of the best proposals (sorted by objectness score) to keep after\n","      non-maximum suppression.\n","\n","    Returns\n","    -------\n","    torch.Tensor, torch.Tensor, torch.Tensor\n","      - Objectness scores (batch_size, height, width, num_anchors)\n","      - Box regressions (batch_size, height, width, num_anchors * 4), as box\n","        deltas (that is, (ty, tx, th, tw) for each anchor)\n","      - Proposals (N, 4) -- all corresponding proposal box corners stored as\n","        (y1, x1, y2, x2).\n","    \"\"\"\n","\n","    # Pass through the network\n","    y = F.relu(self._rpn_conv1(feature_map))\n","    objectness_score_map = t.sigmoid(self._rpn_class(y))\n","    box_deltas_map = self._rpn_boxes(y)\n","\n","    # Transpose shapes to be more convenient:\n","    #   objectness_score_map -> (batch_size, height, width, num_anchors)\n","    #   box_deltas_map       -> (batch_size, height, width, num_anchors * 4)\n","    objectness_score_map = objectness_score_map.permute(0, 2, 3, 1).contiguous()\n","    box_deltas_map = box_deltas_map.permute(0, 2, 3, 1).contiguous()\n","\n","    # Extract box deltas and anchors as (N,4) tensors and scores as (N,) list\n","    anchors, objectness_scores, box_deltas = self._extract_valid(\n","      anchor_map = anchor_map,\n","      anchor_valid_map = anchor_valid_map,\n","      objectness_score_map = objectness_score_map,\n","      box_deltas_map = box_deltas_map\n","    )\n","\n","    # Detach from graph to avoid backprop. According to my understanding, this\n","    # should be redundant here because we later take care to detach the\n","    # proposals (in FasterRCNNModel). However, there is a memory leak involving\n","    # t_convert_deltas_to_boxes() if this is not done here. Ultimately, the\n","    # numerical results are not affected. Proposals returned from this function\n","    # are supposed to be constant and are fed into the detector stage. See any\n","    # commit prior to 209141c for an earlier version of the code here that\n","    # performed all operations on CPU using NumPy, which was slightly slower\n","    # but equivalent.\n","    box_deltas = box_deltas.detach()\n","\n","    # Convert regressions to box corners\n","    proposals = t_convert_deltas_to_boxes(\n","      box_deltas = box_deltas,\n","      anchors = t.from_numpy(anchors).cuda(),\n","      box_delta_means = t.tensor([0, 0, 0, 0], dtype = t.float32, device = \"cuda\"),\n","      box_delta_stds = t.tensor([1, 1, 1, 1], dtype = t.float32, device = \"cuda\")\n","    )\n","\n","    # Keep only the top-N scores. Note that we do not care whether the\n","    # proposals were labeled as objects (score > 0.5) and peform a simple\n","    # ranking among all of them. Restricting them has a strong adverse impact\n","    # on training performance.\n","    sorted_indices = t.argsort(objectness_scores)                   # sort in ascending order of objectness score\n","    sorted_indices = sorted_indices.flip(dims = (0,))               # descending order of score\n","    proposals = proposals[sorted_indices][0:max_proposals_pre_nms]  # grab the top-N best proposals\n","    objectness_scores = objectness_scores[sorted_indices][0:max_proposals_pre_nms]  # corresponding scores\n","\n","    # Clip to image boundaries\n","    proposals[:,0:2] = t.clamp(proposals[:,0:2], min = 0)\n","    proposals[:,2] = t.clamp(proposals[:,2], max = image_shape[1])\n","    proposals[:,3] = t.clamp(proposals[:,3], max = image_shape[2])\n","\n","    # Remove anything less than 16 pixels on a side\n","    height = proposals[:,2] - proposals[:,0]\n","    width = proposals[:,3] - proposals[:,1]\n","    idxs = t.where((height >= 16) & (width >= 16))[0]\n","    proposals = proposals[idxs]\n","    objectness_scores = objectness_scores[idxs]\n","\n","    # Perform NMS\n","    idxs = nms(\n","      boxes = proposals,\n","      scores = objectness_scores,\n","      iou_threshold = 0.7\n","    )\n","    idxs = idxs[0:max_proposals_post_nms]\n","    proposals = proposals[idxs]\n","\n","    # Return network outputs as PyTorch tensors and extracted object proposals\n","    return objectness_score_map, box_deltas_map, proposals\n","\n","  def _extract_valid(self, anchor_map, anchor_valid_map, objectness_score_map, box_deltas_map):\n","    assert objectness_score_map.shape[0] == 1 # only batch size of 1 supported for now\n","\n","    height, width, num_anchors = anchor_valid_map.shape\n","    anchors = anchor_map.reshape((height * width * num_anchors, 4))             # [N,4] all anchors\n","    anchors_valid = anchor_valid_map.reshape((height * width * num_anchors))    # [N,] whether anchors are valid (i.e., do not cross image boundaries)\n","    scores = objectness_score_map.reshape((height * width * num_anchors))       # [N,] prediced objectness scores\n","    box_deltas = box_deltas_map.reshape((height * width * num_anchors, 4))      # [N,4] predicted box delta regression targets\n","\n","    if self._allow_edge_proposals:\n","      # Use all proposals\n","      return anchors, scores, box_deltas\n","    else:\n","      # Filter out those proposals generated at invalid anchors\n","      idxs = anchors_valid > 0\n","      return anchors[idxs], scores[idxs], box_deltas[idxs]\n","\n","\n","def rpn_class_loss_f(predicted_scores, y_true):\n","  \"\"\"\n","  Computes RPN class loss.\n","\n","  Parameters\n","  ----------\n","  predicted_scores : torch.Tensor\n","    A tensor of shape (batch_size, height, width, num_anchors) containing\n","    objectness scores (0 = background, 1 = object).\n","  y_true : torch.Tensor\n","    Ground truth tensor of shape (batch_size, height, width, num_anchors, 6).\n","\n","  Returns\n","  -------\n","  torch.Tensor\n","    Scalar loss.\n","  \"\"\"\n","\n","  epsilon = 1e-7\n","\n","  # y_true_class: (batch_size, height, width, num_anchors), same as predicted_scores\n","  y_true_class = y_true[:,:,:,:,1].reshape(predicted_scores.shape)\n","  y_predicted_class = predicted_scores\n","\n","  # y_mask: y_true[:,:,:,0] is 1.0 for anchors included in the mini-batch\n","  y_mask = y_true[:,:,:,:,0].reshape(predicted_scores.shape)\n","\n","  # Compute how many anchors are actually used in the mini-batch (e.g.,\n","  # typically 256)\n","  N_cls = t.count_nonzero(y_mask) + epsilon\n","\n","  # Compute element-wise loss for all anchors\n","  loss_all_anchors = F.binary_cross_entropy(input = y_predicted_class, target = y_true_class, reduction = \"none\")\n","\n","  # Zero out the ones which should not have been included\n","  relevant_loss_terms = y_mask * loss_all_anchors\n","\n","  # Sum the total loss and normalize by the number of anchors used\n","  return t.sum(relevant_loss_terms) / N_cls\n","\n","def rpn_regression_loss_f(predicted_box_deltas, y_true):\n","  \"\"\"\n","  Computes RPN box delta regression loss.\n","\n","  Parameters\n","  ----------\n","  predicted_box_deltas : torch.Tensor\n","    A tensor of shape (batch_size, height, width, num_anchors * 4) containing\n","    RoI box delta regressions for each anchor, stored as: ty, tx, th, tw.\n","  y_true : torch.Tensor\n","    Ground truth tensor of shape (batch_size, height, width, num_anchors, 6).\n","\n","  Returns\n","  -------\n","  torch.Tensor\n","    Scalar loss.\n","  \"\"\"\n","  epsilon = 1e-7\n","  scale_factor = 1.0  # hyper-parameter that controls magnitude of regression loss and is chosen to make regression term comparable to class term\n","  sigma = 3.0         # see: https://github.com/rbgirshick/py-faster-rcnn/issues/89\n","  sigma_squared = sigma * sigma\n","\n","  y_predicted_regression = predicted_box_deltas\n","  y_true_regression = y_true[:,:,:,:,2:6].reshape(y_predicted_regression.shape)\n","\n","  # Include only anchors that are used in the mini-batch and which correspond\n","  # to objects (positive samples)\n","  y_included = y_true[:,:,:,:,0].reshape(y_true.shape[0:4]) # trainable anchors map: (batch_size, height, width, num_anchors)\n","  y_positive = y_true[:,:,:,:,1].reshape(y_true.shape[0:4]) # positive anchors\n","  y_mask = y_included * y_positive\n","\n","  # y_mask is of the wrong shape. We have one value per (y,x,k) position but in\n","  # fact need to have 4 values (one for each of the regression variables). For\n","  # example, y_predicted might be (1,37,50,36) and y_mask will be (1,37,50,9).\n","  # We need to repeat the last dimension 4 times.\n","  y_mask = y_mask.repeat_interleave(repeats = 4, dim = 3)\n","\n","  # The paper normalizes by dividing by a quantity called N_reg, which is equal\n","  # to the total number of anchors (~2400) and then multiplying by lambda=10.\n","  # This does not make sense to me because we are summing over a mini-batch at\n","  # most, so we use N_cls here. I might be misunderstanding what is going on\n","  # but 10/2400 = 1/240 which is pretty close to 1/256 and the paper mentions\n","  # that training is relatively insensitve to choice of normalization.\n","  N_cls = t.count_nonzero(y_included) + epsilon\n","\n","  # Compute element-wise loss using robust L1 function for all 4 regression\n","  # components\n","  x = y_true_regression - y_predicted_regression\n","  x_abs = t.abs(x)\n","  is_negative_branch = (x_abs < (1.0 / sigma_squared)).float()\n","  R_negative_branch = 0.5 * x * x * sigma_squared\n","  R_positive_branch = x_abs - 0.5 / sigma_squared\n","  loss_all_anchors = is_negative_branch * R_negative_branch + (1.0 - is_negative_branch) * R_positive_branch\n","\n","  # Zero out the ones which should not have been included\n","  relevant_loss_terms = y_mask * loss_all_anchors\n","  return scale_factor * t.sum(relevant_loss_terms) / N_cls\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fsAbgxQ9eDS4"},"outputs":[],"source":["# PyTorch implementation of the final detector stage of Faster R-CNN. As input,\n","# takes a series of proposals (or RoIs) and produces classifications and boxes.\n","# The boxes are parameterized as modifications to the original incoming\n","# proposal boxes. That is, the proposal boxes are exactly analogous to the\n","# anchors that the RPN stage uses.\n","#\n","\n","import torch as t\n","from torch import nn\n","from torch.nn import functional as F\n","from torchvision.ops import RoIPool\n","from torchvision.models import vgg16\n","\n","\n","class DetectorNetwork(nn.Module):\n","  def __init__(self, num_classes, backbone):\n","    super().__init__()\n","\n","    self._input_features = 7 * 7 * backbone.feature_map_channels\n","\n","    # Define network\n","    self._roi_pool = RoIPool(output_size = (7, 7), spatial_scale = 1.0 / backbone.feature_pixels)\n","    self._pool_to_feature_vector = backbone.pool_to_feature_vector\n","    self._classifier = nn.Linear(in_features = backbone.feature_vector_size, out_features = num_classes)\n","    self._regressor = nn.Linear(in_features = backbone.feature_vector_size, out_features = (num_classes - 1) * 4)\n","\n","    # Initialize weights\n","    self._classifier.weight.data.normal_(mean = 0.0, std = 0.01)\n","    self._classifier.bias.data.zero_()\n","    self._regressor.weight.data.normal_(mean = 0.0, std = 0.001)\n","    self._regressor.bias.data.zero_()\n","\n","  def forward(self, feature_map, proposals):\n","    \"\"\"\n","    Predict final class and box delta regressions for region-of-interest\n","    proposals. The proposals serve as \"anchors\" for the box deltas, which\n","    refine the proposals into final boxes.\n","\n","    Parameters\n","    ----------\n","    feature_map : torch.Tensor\n","      Feature map of shape (batch_size, feature_map_channels, height, width).\n","    proposals : torch.Tensor\n","      Region-of-interest box proposals that are likely to contain objects.\n","      Has shape (N, 4), where N is the number of proposals, with each box given\n","      as (y1, x1, y2, x2) in pixel coordinates.\n","\n","    Returns\n","    -------\n","    torch.Tensor, torch.Tensor\n","      Predicted classes, (N, num_classes), encoded as a one-hot vector, and\n","      predicted box delta regressions, (N, 4*(num_classes-1)), where the deltas\n","      are expressed as (ty, tx, th, tw) and are relative to each corresponding\n","      proposal box. Because there is no box for the background class 0, it is\n","      excluded entirely and only (num_classes-1) sets of box delta targets are\n","      computed.\n","    \"\"\"\n","    # Batch size of one for now, so no need to associate proposals with batches\n","    assert feature_map.shape[0] == 1, \"Batch size must be 1\"\n","    batch_idxs = t.zeros((proposals.shape[0], 1)).cuda()\n","\n","    # (N, 5) tensor of (batch_idx, x1, y1, x2, y2)\n","    indexed_proposals = t.cat([ batch_idxs, proposals ], dim = 1)\n","    indexed_proposals = indexed_proposals[:, [ 0, 2, 1, 4, 3 ]] # each row, (batch_idx, y1, x1, y2, x2) -> (batch_idx, x1, y1, x2, y2)\n","\n","    # RoI pooling: (N, feature_map_channels, 7, 7)\n","    rois = self._roi_pool(feature_map, indexed_proposals)\n","\n","    # Forward propagate\n","    y = self._pool_to_feature_vector(rois = rois)\n","    classes_raw = self._classifier(y)\n","    classes = F.softmax(classes_raw, dim = 1)\n","    box_deltas = self._regressor(y)\n","\n","    return classes, box_deltas\n","\n","\n","def detector_class_loss_f(predicted_classes, y_true):\n","  \"\"\"\n","  Computes detector class loss.\n","\n","  Parameters\n","  ----------\n","  predicted_classes : torch.Tensor\n","    RoI predicted classes as categorical vectors, (N, num_classes).\n","  y_true : torch.Tensor\n","    RoI class labels as categorical vectors, (N, num_classes).\n","\n","  Returns\n","  -------\n","  torch.Tensor\n","    Scalar loss.\n","  \"\"\"\n","  epsilon = 1e-7\n","  scale_factor = 1.0\n","  cross_entropy_per_row = -(y_true * t.log(predicted_classes + epsilon)).sum(dim = 1)\n","  N = cross_entropy_per_row.shape[0] + epsilon\n","  cross_entropy = t.sum(cross_entropy_per_row) / N\n","  return scale_factor * cross_entropy\n","\n","def detector_regression_loss_f(predicted_box_deltas, y_true):\n","  \"\"\"\n","  Computes detector regression loss.\n","\n","  Parameters\n","  ----------\n","  predicted_box_deltas : torch.Tensor\n","    RoI predicted box delta regressions, (N, 4*(num_classes-1)). The background\n","    class is excluded and only the non-background classes are included. Each\n","    set of box deltas is stored in parameterized form as (ty, tx, th, tw).\n","  y_true : torch.Tensor\n","    RoI box delta regression ground truth labels, (N, 2, 4*(num_classes-1)).\n","    These are stored as mask values (1 or 0) in (:,0,:) and regression\n","    parameters in (:,1,:). Note that it is important to mask off the predicted\n","    and ground truth values because they may be set to invalid values.\n","\n","  Returns\n","  -------\n","  torch.Tensor\n","    Scalar loss.\n","  \"\"\"\n","  epsilon = 1e-7\n","  scale_factor = 1.0\n","  sigma = 1.0\n","  sigma_squared = sigma * sigma\n","\n","  # We want to unpack the regression targets and the mask of valid targets into\n","  # tensors each of the same shape as the predicted:\n","  #   (num_proposals, 4*(num_classes-1))\n","  # y_true has shape:\n","  #   (num_proposals, 2, 4*(num_classes-1))\n","  y_mask = y_true[:,0,:]\n","  y_true_targets = y_true[:,1,:]\n","\n","  # Compute element-wise loss using robust L1 function for all 4 regression\n","  # targets\n","  x = y_true_targets - predicted_box_deltas\n","  x_abs = t.abs(x)\n","  is_negative_branch = (x_abs < (1.0 / sigma_squared)).float()\n","  R_negative_branch = 0.5 * x * x * sigma_squared\n","  R_positive_branch = x_abs - 0.5 / sigma_squared\n","  losses = is_negative_branch * R_negative_branch + (1.0 - is_negative_branch) * R_positive_branch\n","\n","  # Normalize to number of proposals (e.g., 128). Although this may not be\n","  # what the paper does, it seems to work. Other implemetnations do this.\n","  # Using e.g., the number of positive proposals will cause the loss to\n","  # behave erratically because sometimes N will become very small.\n","  N = y_true.shape[0] + epsilon\n","  relevant_loss_terms = y_mask * losses\n","  return scale_factor * t.sum(relevant_loss_terms) / N\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EpEMEZ4meDHI"},"outputs":[],"source":["# PyTorch implementation of Faster R-CNN training and inference models. Here,\n","# all stages of Faster R-CNN are instantiated, RPN mini-batches are sampled,\n","# ground truth labels from RPN proposal boxes (RoIs) for the detector stage are\n","# generated, and  proposals are sampled.\n","\n","from dataclasses import dataclass\n","import numpy as np\n","import random\n","import torch as t\n","from torch import nn\n","from torchvision.ops import nms\n","\n","class FasterRCNNModel(nn.Module):\n","  @dataclass\n","  class Loss:\n","    rpn_class:            float\n","    rpn_regression:       float\n","    detector_class:       float\n","    detector_regression:  float\n","    total:                float\n","\n","  def __init__(self, num_classes, backbone, rpn_minibatch_size = 256, proposal_batch_size = 128, allow_edge_proposals = True):\n","    \"\"\"\n","    Parameters\n","    ----------\n","    num_classes : int\n","      Number of output classes.\n","    backbone : models.Backbone\n","      Backbone network for feature extraction and pooled feature vector\n","      construction (for input to detector heads).\n","    rpn_minibatch_size : int\n","      Size of the RPN mini-batch. The number of ground truth anchors sampled\n","      for training at each step.\n","    proposal_batch_size : int\n","      Number of region proposals to sample at each training step.\n","    allow_edge_proposals : bool\n","      Whether to use proposals generated at invalid anchors (those that\n","      straddle image edges). Invalid anchors are excluded from RPN training, as\n","      explicitly stated in the literature, but Faster R-CNN implementations\n","      tend to still pass proposals generated at invalid anchors to the\n","      detector.\n","    \"\"\"\n","    super().__init__()\n","\n","    # Constants\n","    self._num_classes = num_classes\n","    self._rpn_minibatch_size = rpn_minibatch_size\n","    self._proposal_batch_size = proposal_batch_size\n","    self._detector_box_delta_means = [ 0, 0, 0, 0 ]\n","    self._detector_box_delta_stds = [ 0.1, 0.1, 0.2, 0.2 ]\n","\n","    # Backbone\n","    self.backbone = backbone\n","\n","    # Network stages\n","    self._stage1_feature_extractor = backbone.feature_extractor\n","    self._stage2_region_proposal_network = RegionProposalNetwork( feature_map_channels = backbone.feature_map_channels, allow_edge_proposals = allow_edge_proposals)\n","    self._stage3_detector_network = DetectorNetwork(num_classes = num_classes, backbone = backbone)\n","\n","  def forward(self, image_data, anchor_map = None, anchor_valid_map = None):\n","    \"\"\"\n","    Forward inference. Use for test and evaluation only.\n","\n","    Parameters\n","    ----------\n","    image_data : torch.Tensor\n","      A tensor of shape (batch_size, channels, height, width) representing\n","      images normalized using the VGG-16 convention (BGR, ImageNet channel-wise\n","      mean-centered).\n","    anchor_map : torch.Tensor\n","      Map of anchors, shaped (height, width, num_anchors * 4). The last\n","      dimension contains the anchor boxes specified as a 4-tuple of\n","      (center_y, center_x, height, width), repeated for all anchors at that\n","      coordinate of the feature map. If this or anchor_valid_map is not\n","      provided, both will be computed here.\n","    anchor_valid_map : torch.Tensor\n","      Map indicating which anchors are valid (do not intersect image bounds),\n","      shaped (height, width). If this or anchor_map is not provided, both will\n","      be computed here.\n","\n","    Returns\n","    -------\n","    np.ndarray, torch.Tensor, torch.Tensor\n","      - Proposals (N, 4) from region proposal network\n","      - Classes (M, num_classes) from detector network\n","      - Box delta regressions (M, (num_classes - 1) * 4) from detector network\n","    \"\"\"\n","    assert image_data.shape[0] == 1, \"Batch size must be 1\"\n","    image_shape = image_data.shape[1:]  # (batch_index, channels, height, width) -> (channels, height, width)\n","\n","    # Anchor maps can be pre-computed and passed in explicitly (for performance\n","    # reasons) but if they are missing, we compute them on-the-fly here\n","    if anchor_map is None or anchor_valid_map is None:\n","      feature_map_shape = self.backbone.compute_feature_map_shape(image_shape = image_shape)\n","      anchor_map, anchor_valid_map = anchors.generate_anchor_maps(image_shape = image_shape, feature_map_shape = feature_map_shape, feature_pixels = self.backbone.feature_pixels)\n","\n","    # Run each stage\n","    feature_map = self._stage1_feature_extractor(image_data = image_data)\n","    objectness_score_map, box_deltas_map, proposals = self._stage2_region_proposal_network(\n","      feature_map = feature_map,\n","      image_shape = image_shape,\n","      anchor_map = anchor_map,\n","      anchor_valid_map = anchor_valid_map,\n","      max_proposals_pre_nms = 6000, # test time values\n","      max_proposals_post_nms = 300\n","    )\n","    classes, box_deltas = self._stage3_detector_network(\n","      feature_map = feature_map,\n","      proposals = proposals\n","    )\n","\n","    return proposals, classes, box_deltas\n","\n","  # @utils.no_grad\n","  def predict(self, image_data, score_threshold, anchor_map = None, anchor_valid_map = None):\n","    \"\"\"\n","    Performs inference on an image and obtains the final detected boxes.\n","\n","    Parameters\n","    ----------\n","    image_data : torch.Tensor\n","      A tensor of shape (batch_size, channels, height, width) representing\n","      images normalized using the VGG-16 convention (BGR, ImageNet channel-wise\n","      mean-centered).\n","    score_threshold : float\n","      Minimum required score threshold (applied per class) for a detection to\n","      be considered. Set this higher for visualization to minimize extraneous\n","      boxes.\n","    anchor_map : torch.Tensor\n","      Map of anchors, shaped (height, width, num_anchors * 4). The last\n","      dimension contains the anchor boxes specified as a 4-tuple of\n","      (center_y, center_x, height, width), repeated for all anchors at that\n","      coordinate of the feature map. If this or anchor_valid_map is not\n","      provided, both will be computed here.\n","    anchor_valid_map : torch.Tensor\n","      Map indicating which anchors are valid (do not intersect image bounds),\n","      shaped (height, width). If this or anchor_map is not provided, both will\n","      be computed here.\n","\n","    Returns\n","    -------\n","    Dict[int, np.ndarray]\n","      Scored boxes, (N, 5) tensor of box corners and class score,\n","      (y1, x1, y2, x2, score), indexed by class index.\n","    \"\"\"\n","    self.eval()\n","    assert image_data.shape[0] == 1, \"Batch size must be 1\"\n","\n","    # Forward inference\n","    proposals, classes, box_deltas = self(\n","      image_data = image_data,\n","      anchor_map = anchor_map,\n","      anchor_valid_map = anchor_valid_map\n","    )\n","    proposals = proposals.cpu().numpy()\n","    classes = classes.cpu().numpy()\n","    box_deltas = box_deltas.cpu().numpy()\n","\n","    # Convert proposal boxes -> center point and size\n","    proposal_anchors = np.empty(proposals.shape)\n","    proposal_anchors[:,0] = 0.5 * (proposals[:,0] + proposals[:,2]) # center_y\n","    proposal_anchors[:,1] = 0.5 * (proposals[:,1] + proposals[:,3]) # center_x\n","    proposal_anchors[:,2:4] = proposals[:,2:4] - proposals[:,0:2]   # height, width\n","\n","    # Separate out results per class: class_idx -> (y1, x1, y2, x2, score)\n","    boxes_and_scores_by_class_idx = {}\n","    for class_idx in range(1, classes.shape[1]):  # skip class 0 (background)\n","      # Get the box deltas (ty, tx, th, tw) corresponding to this class, for\n","      # all proposals\n","      box_delta_idx = (class_idx - 1) * 4\n","      box_delta_params = box_deltas[:, (box_delta_idx + 0) : (box_delta_idx + 4)] # (N, 4)\n","      proposal_boxes_this_class = convert_deltas_to_boxes(\n","        box_deltas = box_delta_params,\n","        anchors = proposal_anchors,\n","        box_delta_means = self._detector_box_delta_means,\n","        box_delta_stds = self._detector_box_delta_stds\n","      )\n","\n","      # Clip to image boundaries\n","      proposal_boxes_this_class[:,0::2] = np.clip(proposal_boxes_this_class[:,0::2], 0, image_data.shape[2] - 1)  # clip y1 and y2 to [0,height)\n","      proposal_boxes_this_class[:,1::2] = np.clip(proposal_boxes_this_class[:,1::2], 0, image_data.shape[3] - 1)  # clip x1 and x2 to [0,width)\n","\n","      # Get the scores for this class. The class scores are returned in\n","      # normalized categorical form. Each row corresponds to a class.\n","      scores_this_class = classes[:,class_idx]\n","\n","      # Keep only those scoring high enough\n","      sufficiently_scoring_idxs = np.where(scores_this_class > score_threshold)[0]\n","      proposal_boxes_this_class = proposal_boxes_this_class[sufficiently_scoring_idxs]\n","      scores_this_class = scores_this_class[sufficiently_scoring_idxs]\n","      boxes_and_scores_by_class_idx[class_idx] = (proposal_boxes_this_class, scores_this_class)\n","\n","    # Perform NMS per class\n","    scored_boxes_by_class_idx = {}\n","    for class_idx, (boxes, scores) in boxes_and_scores_by_class_idx.items():\n","      idxs = nms(\n","        boxes = t.from_numpy(boxes).cuda(),\n","        scores = t.from_numpy(scores).cuda(),\n","        iou_threshold = 0.3 #TODO: unsure about this. Paper seems to imply 0.5 but https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/fast_rcnn/config.py has 0.3 for test NMS\n","      ).cpu().numpy()\n","      boxes = boxes[idxs]\n","      scores = np.expand_dims(scores[idxs], axis = 0) # (N,) -> (N,1)\n","      scored_boxes = np.hstack([ boxes, scores.T ])   # (N,5), with each row: (y1, x1, y2, x2, score)\n","      scored_boxes_by_class_idx[class_idx] = scored_boxes\n","\n","    return scored_boxes_by_class_idx\n","\n","  def train_step(self, optimizer, image_data, anchor_map, anchor_valid_map, gt_rpn_map, gt_rpn_object_indices, gt_rpn_background_indices, gt_boxes):\n","    \"\"\"\n","    Performs one training step on a sample of data.\n","\n","    Parameters\n","    ----------\n","    optimizer : torch.optim.Optimizer\n","      Optimizer.\n","    image_data : torch.Tensor\n","      A tensor of shape (batch_size, channels, height, width) representing\n","      images normalized using the VGG-16 convention (BGR, ImageNet channel-wise\n","      mean-centered).\n","    anchor_map : torch.Tensor\n","      Map of anchors, shaped (height, width, num_anchors * 4). The last\n","      dimension contains the anchor boxes specified as a 4-tuple of\n","      (center_y, center_x, height, width), repeated for all anchors at that\n","      coordinate of the feature map. If this or anchor_valid_map is not\n","      provided, both will be computed here.\n","    anchor_valid_map : torch.Tensor\n","      Map indicating which anchors are valid (do not intersect image bounds),\n","      shaped (height, width). If this or anchor_map is not provided, both will\n","      be computed here.\n","    gt_rpn_map : torch.Tensor\n","      Ground truth RPN map of shape\n","      (batch_size, height, width, num_anchors, 6), where height and width are\n","      the feature map dimensions, not the input image dimensions. The final\n","      dimension contains:\n","       - 0: Trainable anchor (1) or not (0). Only valid and non-neutral (that\n","            is, definitely positive or negative) anchors are trainable. This is\n","            the same as anchor_valid_map with additional invalid anchors caused\n","            by neutral samples\n","       - 1: For trainable anchors, whether the anchor is an object anchor (1)\n","            or background anchor (0). For non-trainable anchors, will be 0.\n","       - 2: Regression target for box center, ty.\n","       - 3: Regression target for box center, tx.\n","       - 4: Regression target for box size, th.\n","       - 5: Regression target for box size, tw.\n","    gt_rpn_object_indices : List[np.ndarray]\n","      For each image in the batch, a map of shape (N, 3) of indices (y, x, k)\n","      of all N object anchors in the RPN ground truth map.\n","    gt_rpn_background_indices : List[np.ndarray]\n","      For each image in the batch, a map of shape (M, 3) of indices of all M\n","      background anchors in the RPN ground truth map.\n","    gt_boxes : List[List[datasets.training_sample.Box]]\n","      For each image in the batch, a list of ground truth object boxes.\n","\n","    Returns\n","    -------\n","    Loss\n","      Loss (a dataclass with class and regression losses for both the RPN and\n","      detector states).\n","    \"\"\"\n","    self.train()\n","\n","    # Clear accumulated gradient\n","    optimizer.zero_grad()\n","\n","    # For now, we only support a batch size of 1\n","    assert image_data.shape[0] == 1, \"Batch size must be 1\"\n","    assert len(gt_rpn_map.shape) == 5 and gt_rpn_map.shape[0] == 1, \"Batch size must be 1\"\n","    assert len(gt_rpn_object_indices) == 1, \"Batch size must be 1\"\n","    assert len(gt_rpn_background_indices) == 1, \"Batch size must be 1\"\n","    assert len(gt_boxes) == 1, \"Batch size must be 1\"\n","    image_shape = image_data.shape[1:]\n","\n","    # Stage 1: Extract features\n","    feature_map = self._stage1_feature_extractor(image_data = image_data)\n","\n","    # Stage 2: Generate object proposals using RPN\n","    rpn_score_map, rpn_box_deltas_map, proposals = self._stage2_region_proposal_network(\n","      feature_map = feature_map,\n","      image_shape = image_shape,  # each image in batch has identical shape: (num_channels, height, width)\n","      anchor_map = anchor_map,\n","      anchor_valid_map = anchor_valid_map,\n","      max_proposals_pre_nms = 12000,\n","      max_proposals_post_nms = 2000\n","    )\n","\n","    # Sample random mini-batch of anchors (for RPN training)\n","    gt_rpn_minibatch_map = self._sample_rpn_minibatch(\n","      rpn_map = gt_rpn_map,\n","      object_indices = gt_rpn_object_indices,\n","      background_indices = gt_rpn_background_indices\n","    )\n","\n","    # Assign labels to proposals and take random sample (for detector training)\n","    proposals, gt_classes, gt_box_deltas = self._label_proposals(\n","      proposals = proposals,\n","      gt_boxes = gt_boxes[0], # for now, batch size of 1\n","      min_background_iou_threshold = 0.0,\n","      min_object_iou_threshold = 0.5\n","    )\n","    proposals, gt_classes, gt_box_deltas = self._sample_proposals(\n","      proposals = proposals,\n","      gt_classes = gt_classes,\n","      gt_box_deltas = gt_box_deltas,\n","      max_proposals = self._proposal_batch_size,\n","      positive_fraction = 0.25\n","    )\n","\n","    # Make sure RoI proposals and ground truths are detached from computational\n","    # graph so that gradients are not propagated through them. They are treated\n","    # as constant inputs into the detector stage.\n","    proposals = proposals.detach()\n","    gt_classes = gt_classes.detach()\n","    gt_box_deltas = gt_box_deltas.detach()\n","\n","    # Stage 3: Detector\n","    detector_classes, detector_box_deltas = self._stage3_detector_network(\n","      feature_map = feature_map,\n","      proposals = proposals\n","    )\n","\n","    # Compute losses\n","    rpn_class_loss = rpn_class_loss_f(predicted_scores = rpn_score_map, y_true = gt_rpn_minibatch_map)\n","    rpn_regression_loss = rpn_regression_loss_f(predicted_box_deltas = rpn_box_deltas_map, y_true = gt_rpn_minibatch_map)\n","    detector_class_loss = detector_class_loss_f(predicted_classes = detector_classes, y_true = gt_classes)\n","    detector_regression_loss = detector_regression_loss_f(predicted_box_deltas = detector_box_deltas, y_true = gt_box_deltas)\n","    total_loss = rpn_class_loss + rpn_regression_loss + detector_class_loss + detector_regression_loss\n","    loss = FasterRCNNModel.Loss(\n","      rpn_class = rpn_class_loss.detach().cpu().item(),\n","      rpn_regression = rpn_regression_loss.detach().cpu().item(),\n","      detector_class = detector_class_loss.detach().cpu().item(),\n","      detector_regression = detector_regression_loss.detach().cpu().item(),\n","      total = total_loss.detach().cpu().item()\n","    )\n","\n","    # Backprop\n","    total_loss.backward()\n","\n","    # Optimizer step\n","    optimizer.step()\n","\n","    # Return losses and data useful for computing statistics\n","    return loss\n","\n","  def _sample_rpn_minibatch(self, rpn_map, object_indices, background_indices):\n","    \"\"\"\n","    Selects anchors for training and produces a copy of the RPN ground truth\n","    map with only those anchors marked as trainable.\n","\n","    Parameters\n","    ----------\n","    rpn_map : np.ndarray\n","      RPN ground truth map of shape\n","      (batch_size, height, width, num_anchors, 6).\n","    object_indices : List[np.ndarray]\n","      For each image in the batch, a map of shape (N, 3) of indices (y, x, k)\n","      of all N object anchors in the RPN ground truth map.\n","    background_indices : List[np.ndarray]\n","      For each image in the batch, a map of shape (M, 3) of indices of all M\n","      background anchors in the RPN ground truth map.\n","\n","    Returns\n","    -------\n","    np.ndarray\n","      A copy of the RPN ground truth map with index 0 of the last dimension\n","      recomputed to include only anchors in the minibatch.\n","    \"\"\"\n","    assert rpn_map.shape[0] == 1, \"Batch size must be 1\"\n","    assert len(object_indices) == 1, \"Batch size must be 1\"\n","    assert len(background_indices) == 1, \"Batch size must be 1\"\n","    positive_anchors = object_indices[0]\n","    negative_anchors = background_indices[0]\n","    assert len(positive_anchors) + len(negative_anchors) >= self._rpn_minibatch_size, \"Image has insufficient anchors for RPN minibatch size of %d\" % self._rpn_minibatch_size\n","    assert len(positive_anchors) > 0, \"Image does not have any positive anchors\"\n","    assert self._rpn_minibatch_size % 2 == 0, \"RPN minibatch size must be evenly divisible\"\n","\n","    # Sample, producing indices into the index maps\n","    num_positive_anchors = len(positive_anchors)\n","    num_negative_anchors = len(negative_anchors)\n","    num_positive_samples = min(self._rpn_minibatch_size // 2, num_positive_anchors) # up to half the samples should be positive, if possible\n","    num_negative_samples = self._rpn_minibatch_size - num_positive_samples          # the rest should be negative\n","    positive_anchor_idxs = random.sample(range(num_positive_anchors), num_positive_samples)\n","    negative_anchor_idxs = random.sample(range(num_negative_anchors), num_negative_samples)\n","\n","    # Construct index expressions into RPN map\n","    positive_anchors = positive_anchors[positive_anchor_idxs]\n","    negative_anchors = negative_anchors[negative_anchor_idxs]\n","    trainable_anchors = np.concatenate([ positive_anchors, negative_anchors ])\n","    batch_idxs = np.zeros(len(trainable_anchors))\n","    trainable_idxs = (batch_idxs, trainable_anchors[:,0], trainable_anchors[:,1], trainable_anchors[:,2], 0)\n","\n","    # Create a copy of the RPN map with samples set as trainable\n","    rpn_minibatch_map = rpn_map.clone()\n","    rpn_minibatch_map[:,:,:,:,0] = 0\n","    rpn_minibatch_map[trainable_idxs] = 1\n","\n","    return rpn_minibatch_map\n","\n","  def _label_proposals(self, proposals, gt_boxes, min_background_iou_threshold, min_object_iou_threshold):\n","    \"\"\"\n","    Determines which proposals generated by the RPN stage overlap with ground\n","    truth boxes and creates ground truth labels for the subsequent detector\n","    stage.\n","\n","    Parameters\n","    ----------\n","    proposals : torch.Tensor\n","      Proposal corners, shaped (N, 4).\n","    gt_boxes : List[datasets.training_sample.Box]\n","      Ground truth object boxes.\n","    min_background_iou_threshold : float\n","      Minimum IoU threshold with ground truth boxes below which proposals are\n","      ignored entirely. Proposals with an IoU threshold in the range\n","      [min_background_iou_threshold, min_object_iou_threshold) are labeled as\n","      background. This value can be greater than 0, which has the effect of\n","      selecting more difficult background examples that have some degree of\n","      overlap with ground truth boxes.\n","    min_object_iou_threshold : float\n","      Minimum IoU threshold for a proposal to be labeled as an object.\n","\n","    Returns\n","    -------\n","    torch.Tensor, torch.Tensor, torch.Tensor\n","      Proposals, (N, 4), labeled as either objects or background (depending on\n","      IoU thresholds, some proposals can end up as neither and are excluded\n","      here); one-hot encoded class labels, (N, num_classes), for each proposal;\n","      and box delta regression targets, (N, 2, (num_classes - 1) * 4), for each\n","      proposal. Box delta target values are present at locations [:,1,:] and\n","      consist of (ty, tx, th, tw) for the class that the box corresponds to.\n","      The entries for all other classes and the background classes should be\n","      ignored. A mask is written to locations [:,0,:]. For each proposal\n","      assigned a non-background class, there will be 4 consecutive elements\n","      marked with 1 indicating the corresponding box delta target values are to\n","      be used. There are no box delta regression targets for background\n","      proposals and the mask is entirely 0 for those proposals.\n","    \"\"\"\n","    assert min_background_iou_threshold < min_object_iou_threshold, \"Object threshold must be greater than background threshold\"\n","\n","    # Convert ground truth box corners to (M,4) tensor and class indices to (M,)\n","    gt_box_corners = np.array([ box.corners for box in gt_boxes ], dtype = np.float32)\n","    gt_box_corners = t.from_numpy(gt_box_corners).cuda()\n","    gt_box_class_idxs = t.tensor([ box.class_index for box in gt_boxes ], dtype = t.long, device = \"cuda\")\n","\n","    # Let's be crafty and create some fake proposals that match the ground\n","    # truth boxes exactly. This isn't strictly necessary and the model should\n","    # work without it but it will help training and will ensure that there are\n","    # always some positive examples to train on.\n","    proposals = t.vstack([ proposals, gt_box_corners ])\n","\n","    # Compute IoU between each proposal (N,4) and each ground truth box (M,4)\n","    # -> (N, M)\n","    ious = t_intersection_over_union(boxes1 = proposals, boxes2 = gt_box_corners)\n","\n","    # Find the best IoU for each proposal, the class of the ground truth box\n","    # associated with it, and the box corners\n","    best_ious = t.max(ious, dim = 1).values         # (N,) of maximum IoUs for each of the N proposals\n","    box_idxs = t.argmax(ious, dim = 1)              # (N,) of ground truth box index for each proposal\n","    gt_box_class_idxs = gt_box_class_idxs[box_idxs] # (N,) of class indices of highest-IoU box for each proposal\n","    gt_box_corners = gt_box_corners[box_idxs]       # (N,4) of box corners of highest-IoU box for each proposal\n","\n","    # Remove all proposals whose best IoU is less than the minimum threshold\n","    # for a negative (background) sample. We also check for IoUs > 0 because\n","    # due to earlier clipping, we may get invalid 0-area proposals.\n","    idxs = t.where((best_ious >= min_background_iou_threshold))[0]  # keep proposals w/ sufficiently high IoU\n","    proposals = proposals[idxs]\n","    best_ious = best_ious[idxs]\n","    gt_box_class_idxs = gt_box_class_idxs[idxs]\n","    gt_box_corners = gt_box_corners[idxs]\n","\n","    # IoUs less than min_object_iou_threshold will be labeled as background\n","    gt_box_class_idxs[best_ious < min_object_iou_threshold] = 0\n","\n","    # One-hot encode class labels\n","    num_proposals = proposals.shape[0]\n","    gt_classes = t.zeros((num_proposals, self._num_classes), dtype = t.float32, device = \"cuda\")  # (N,num_classes)\n","    gt_classes[ t.arange(num_proposals), gt_box_class_idxs ] = 1.0\n","\n","    # Convert proposals and ground truth boxes into \"anchor\" format (center\n","    # points and side lengths). For the detector stage, the proposals serve as\n","    # the anchors relative to which the final box predictions will be\n","    # regressed.\n","    proposal_centers = 0.5 * (proposals[:,0:2] + proposals[:,2:4])          # center_y, center_x\n","    proposal_sides = proposals[:,2:4] - proposals[:,0:2]                    # height, width\n","    gt_box_centers = 0.5 * (gt_box_corners[:,0:2] + gt_box_corners[:,2:4])  # center_y, center_x\n","    gt_box_sides = gt_box_corners[:,2:4] - gt_box_corners[:,0:2]            # height, width\n","\n","    # Compute box delta regression targets (ty, tx, th, tw) for each proposal\n","    # based on the best box selected\n","    box_delta_targets = t.empty((num_proposals, 4), dtype = t.float32, device = \"cuda\") # (N,4)\n","    box_delta_targets[:,0:2] = (gt_box_centers - proposal_centers) / proposal_sides # ty = (gt_center_y - proposal_center_y) / proposal_height, tx = (gt_center_x - proposal_center_x) / proposal_width\n","    box_delta_targets[:,2:4] = t.log(gt_box_sides / proposal_sides)                 # th = log(gt_height / proposal_height), tw = (gt_width / proposal_width)\n","    box_delta_means = t.tensor(self._detector_box_delta_means, dtype = t.float32, device = \"cuda\")\n","    box_delta_stds = t.tensor(self._detector_box_delta_stds, dtype = t.float32, device = \"cuda\")\n","    box_delta_targets[:,:] -= box_delta_means                               # mean adjustment\n","    box_delta_targets[:,:] /= box_delta_stds                                # standard deviation scaling\n","\n","    # Convert regression targets into a map of shape (N,2,4*(C-1)) where C is\n","    # the number of classes and [:,0,:] specifies a mask for the corresponding\n","    # target components at [:,1,:]. Targets are ordered (ty, tx, th, tw).\n","    # Background class 0 is not present at all.\n","    gt_box_deltas = t.zeros((num_proposals, 2, 4 * (self._num_classes - 1)), dtype = t.float32, device = \"cuda\")\n","    gt_box_deltas[:,0,:] = t.repeat_interleave(gt_classes, repeats = 4, dim = 1)[:,4:]  # create masks using interleaved repetition, remembering to ignore class 0\n","    gt_box_deltas[:,1,:] = t.tile(box_delta_targets, dims = (1, self._num_classes - 1)) # populate regression targets with straightforward repetition (only those columns corresponding to class are masked on)\n","\n","    return proposals, gt_classes, gt_box_deltas\n","\n","  def _sample_proposals(self, proposals, gt_classes, gt_box_deltas, max_proposals, positive_fraction):\n","    if max_proposals <= 0:\n","      return proposals, gt_classes, gt_box_deltas\n","\n","    # Get positive and negative (background) proposals\n","    class_indices = t.argmax(gt_classes, axis = 1)  # (N,num_classes) -> (N,), where each element is the class index (highest score from its row)\n","    positive_indices = t.where(class_indices > 0)[0]\n","    negative_indices = t.where(class_indices <= 0)[0]\n","    num_positive_proposals = len(positive_indices)\n","    num_negative_proposals = len(negative_indices)\n","\n","    # Select positive and negative samples, if there are enough. Note that the\n","    # number of positive samples can be either the positive fraction of the\n","    # *actual* number of proposals *or* the *desired* number (max_proposals).\n","    # In practice, these yield virtually identical results but the latter\n","    # method will yield slightly more positive samples in the rare cases when\n","    # the number of proposals is below the desired number. Here, we use the\n","    # former method but others, such as Yun Chen, use the latter. To implement\n","    # it, replace num_samples with max_proposals in the line that computes\n","    # num_positive_samples. I am not sure what the original Faster R-CNN\n","    # implementation does.\n","    num_samples = min(max_proposals, len(class_indices))\n","    num_positive_samples = min(round(num_samples * positive_fraction), num_positive_proposals)\n","    num_negative_samples = min(num_samples - num_positive_samples, num_negative_proposals)\n","\n","    # Do we have enough?\n","    if num_positive_samples <= 0 or num_negative_samples <= 0:\n","      return proposals[[]], gt_classes[[]], gt_box_deltas[[]] # return 0-length tensors\n","\n","    # Sample randomly\n","    positive_sample_indices = positive_indices[ t.randperm(len(positive_indices))[0:num_positive_samples] ]\n","    negative_sample_indices = negative_indices[ t.randperm(len(negative_indices))[0:num_negative_samples] ]\n","    indices = t.cat([ positive_sample_indices, negative_sample_indices ])\n","\n","    # Return\n","    return proposals[indices], gt_classes[indices], gt_box_deltas[indices]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OA0LSd8zR71B"},"outputs":[],"source":["# Construct model and load initial weights\n","model = FasterRCNNModel(num_classes = Dataset.num_classes, backbone = backbone).cuda()"]},{"cell_type":"markdown","metadata":{"id":"J-pffYMdiguO"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q6bSqgckDdK7"},"outputs":[],"source":["epochs = 10\n","momentum = 0.9\n","learning_rate =  1e-3\n","weight_decay = 5e-4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eli0sJudiu_P"},"outputs":[],"source":["# creating an optimizer\n","params = []\n","for key, value in dict(model.named_parameters()).items():\n","  if not value.requires_grad:\n","    continue\n","  if \"weight\" in key:\n","    params += [{ \"params\": [value], \"weight_decay\": weight_decay }]\n","\n","optimizer = t.optim.SGD(params, lr = learning_rate, momentum = momentum)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DAHIVPwfiiMw","outputId":"985000c3-06af-4422-d229-4421b1d9b2f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n"]},{"name":"stderr","output_type":"stream","text":["\r  0%|          | 0/5011 [00:00<?, ?it/s, detector_class_loss=inf, detector_regr_loss=inf, rpn_class_loss=inf, rpn_regr_loss=inf, total_loss=inf]<ipython-input-3-14dcf3ba3bf1>:72: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n","  data = imageio.imread(url, pilmode = \"RGB\")\n","100%|██████████| 5011/5011 [13:34<00:00,  6.16it/s, rpn_class_loss=0.1942, rpn_regr_loss=0.0562, detector_class_loss=0.4086, detector_regr_loss=0.3549, total_loss=1.01]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/10\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 5011/5011 [13:38<00:00,  6.13it/s, rpn_class_loss=0.1420, rpn_regr_loss=0.0499, detector_class_loss=0.2937, detector_regr_loss=0.3237, total_loss=0.81]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/10\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 5011/5011 [13:38<00:00,  6.12it/s, rpn_class_loss=0.1255, rpn_regr_loss=0.0474, detector_class_loss=0.2465, detector_regr_loss=0.2795, total_loss=0.70]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/10\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 5011/5011 [13:38<00:00,  6.13it/s, rpn_class_loss=0.1136, rpn_regr_loss=0.0460, detector_class_loss=0.2171, detector_regr_loss=0.2504, total_loss=0.63]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5/10\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 5011/5011 [13:37<00:00,  6.13it/s, rpn_class_loss=0.1056, rpn_regr_loss=0.0444, detector_class_loss=0.1975, detector_regr_loss=0.2305, total_loss=0.58]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6/10\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 5011/5011 [13:39<00:00,  6.11it/s, rpn_class_loss=0.0972, rpn_regr_loss=0.0432, detector_class_loss=0.1796, detector_regr_loss=0.2146, total_loss=0.53]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7/10\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 5011/5011 [13:40<00:00,  6.10it/s, rpn_class_loss=0.0897, rpn_regr_loss=0.0424, detector_class_loss=0.1660, detector_regr_loss=0.1996, total_loss=0.50]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8/10\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 5011/5011 [13:39<00:00,  6.12it/s, rpn_class_loss=0.0845, rpn_regr_loss=0.0411, detector_class_loss=0.1597, detector_regr_loss=0.1888, total_loss=0.47]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9/10\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 5011/5011 [13:38<00:00,  6.12it/s, rpn_class_loss=0.0790, rpn_regr_loss=0.0407, detector_class_loss=0.1493, detector_regr_loss=0.1782, total_loss=0.45]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10/10\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 5011/5011 [13:43<00:00,  6.09it/s, rpn_class_loss=0.0736, rpn_regr_loss=0.0398, detector_class_loss=0.1376, detector_regr_loss=0.1684, total_loss=0.42]\n"]}],"source":["for epoch in range(1, 1 + epochs):\n","  print(\"Epoch %d/%d\" % (epoch, epochs))\n","\n","  rpn_class_loss = float(\"inf\")\n","  rpn_regression_loss = float(\"inf\")\n","  detector_class_loss = float(\"inf\")\n","  detector_regression_loss = float(\"inf\")\n","  _rpn_class_losses = []\n","  _rpn_regression_losses = []\n","  _detector_class_losses = []\n","  _detector_regression_losses = []\n","\n","  progbar = tqdm(iterable = iter(training_data), total = training_data.num_samples, postfix = {\n","      \"rpn_class_loss\": \"%1.4f\" % rpn_class_loss,\n","      \"rpn_regr_loss\": \"%1.4f\" % rpn_regression_loss,\n","      \"detector_class_loss\": \"%1.4f\" % detector_class_loss,\n","      \"detector_regr_loss\": \"%1.4f\" % detector_regression_loss,\n","      \"total_loss\": \"%1.2f\" % (rpn_class_loss + rpn_regression_loss + detector_class_loss + detector_regression_loss)\n","    })\n","\n","  for sample in progbar:\n","    loss = model.train_step(  # don't retain any tensors we don't need (helps memory usage)\n","      optimizer = optimizer,\n","      image_data = t.from_numpy(sample.image_data).unsqueeze(dim = 0).cuda(),\n","      anchor_map = sample.anchor_map,\n","      anchor_valid_map = sample.anchor_valid_map,\n","      gt_rpn_map = t.from_numpy(sample.gt_rpn_map).unsqueeze(dim = 0).cuda(),\n","      gt_rpn_object_indices = [ sample.gt_rpn_object_indices ],\n","      gt_rpn_background_indices = [ sample.gt_rpn_background_indices ],\n","      gt_boxes = [ sample.gt_boxes ]\n","    )\n","\n","\n","    _rpn_class_losses.append(loss.rpn_class)\n","    _rpn_regression_losses.append(loss.rpn_regression)\n","    _detector_class_losses.append(loss.detector_class)\n","    _detector_regression_losses.append(loss.detector_regression)\n","    rpn_class_loss = np.mean(_rpn_class_losses)\n","    rpn_regression_loss = np.mean(_rpn_regression_losses)\n","    detector_class_loss = np.mean(_detector_class_losses)\n","    detector_regression_loss = np.mean(_detector_regression_losses)\n","\n","\n","    progbar.set_postfix({\n","      \"rpn_class_loss\": \"%1.4f\" % rpn_class_loss,\n","      \"rpn_regr_loss\": \"%1.4f\" % rpn_regression_loss,\n","      \"detector_class_loss\": \"%1.4f\" % detector_class_loss,\n","      \"detector_regr_loss\": \"%1.4f\" % detector_regression_loss,\n","      \"total_loss\": \"%1.2f\" % (rpn_class_loss + rpn_regression_loss + detector_class_loss + detector_regression_loss)\n","    })"]},{"cell_type":"markdown","metadata":{"id":"Y8tG6NBTL_A5"},"source":["# Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":162},"executionInfo":{"elapsed":5,"status":"error","timestamp":1710352975467,"user":{"displayName":"Sarvesh Khetan","userId":"05805229168588767785"},"user_tz":-330},"id":"SPNI702DtFFj","outputId":"84e9fc95-a109-4d33-d75f-a80befb139de"},"outputs":[{"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-c1cac2481208>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://trzy.org/files/fasterrcnn/gary.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_preprocessing_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_dimension_pixels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}],"source":["image_data, image_obj, _, _ = load_image(url = \"https://trzy.org/files/fasterrcnn/gary.jpg\", preprocessing = model.backbone.image_preprocessing_params, min_dimension_pixels = 600)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HNNfi2R-tnue"},"outputs":[],"source":["image_data = t.from_numpy(image_data).unsqueeze(dim = 0).cuda()\n","scored_boxes_by_class_index = model.predict(image_data = image_data, score_threshold = 0.7)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gkzLGpZmtcYc"},"outputs":[],"source":["show_detections(\n","                output_path = None,\n","                show_image = True,\n","                image = image_obj,\n","                scored_boxes_by_class_index = scored_boxes_by_class_index,\n","                class_index_to_name = Dataset.class_index_to_name\n","                )"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["TRi_vs4fcdN_","inR2bAvKdYYP","bFYkx6LadU6n","NzaMu5cDdSWS","I_gaxu1LdMli","oPW67aIF3ZDn","tsv-kIZ5HGCE","rZgLZzZyPr9o","J-pffYMdiguO","Y8tG6NBTL_A5"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
