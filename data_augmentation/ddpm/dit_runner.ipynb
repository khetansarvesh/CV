{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/khetansarvesh/CV/blob/main/data_augmentation/ddpm/dit_runner.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qiHWnpIsCARS"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import yaml\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import glob\n",
    "import cv2\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLjIBUkmCBKc"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print('Using mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GeLcFxcuCT_C"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCiafg0eFQOF"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M1F19gZ4FSoU"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) # Define the transformation to normalize the data between 1 and -1 (mean = 0.5 and variance = 0.5 will transform to values between 1 and -1)\n",
    "mnist = datasets.MNIST(root='./data', train=True, transform=transform, download=True) # downloading the MNIST train dataset and then applying some transformations\n",
    "data_loader = DataLoader(dataset=mnist, batch_size=32, shuffle=True) # loading the downloaded dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QB7nTAZdCaSp"
   },
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_embedding(time_steps, # 1D array of timesteps eg [1,10,500,40,300]\n",
    "                       temb_dim): # dimension of vector to which each of these timestep needs to be converted to eg 128\n",
    "\n",
    "    # factor = 10000^(2i/d_model)\n",
    "    factor = 10000 ** ((torch.arange(start=0, end=temb_dim // 2, dtype=torch.float32, device=time_steps.device) / (temb_dim // 2)))\n",
    "\n",
    "    # pos / factor\n",
    "    t_emb = time_steps[:, None].repeat(1, temb_dim // 2) / factor\n",
    "\n",
    "    # now taking sin and cos of t_emb\n",
    "    return torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.ti_1(t_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ae_transformers(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embedding = nn.Sequential(nn.LayerNorm(1*4*4), nn.Linear(1*4*4, 768), nn.LayerNorm(768))\n",
    "        self.position_embedding = nn.Parameter(data=torch.randn(1, 49, 768),requires_grad=True)\n",
    "        self.embedding_dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer=nn.TransformerEncoderLayer(d_model=768,\n",
    "                                                                                                  nhead=2,\n",
    "                                                                                                  dim_feedforward=3072,\n",
    "                                                                                                  activation=\"gelu\",\n",
    "                                                                                                  batch_first=True,\n",
    "                                                                                                  norm_first=True), # Create a single Transformer Encoder Layer\n",
    "                                                        num_layers=2) # Stack it N times\n",
    "\n",
    "        # Final Linear Layer\n",
    "        self.proj_out = nn.Linear(768, 1*4*4)\n",
    "\n",
    "        # Time projection\n",
    "        self.ti_1 = nn.Linear(128, 400)\n",
    "        self.ti_2 = nn.Linear(400, 768)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # getting time embeddings\n",
    "        t_emb = get_time_embedding(torch.as_tensor(t).long(), 128)\n",
    "\n",
    "        # projecting time embeddings to D = 768 dimensions\n",
    "        time_proj1 = self.ti_1(t_emb)\n",
    "        time_proj2 = self.ti_2(time_proj1)\n",
    "\n",
    "        # 32, 1, 28, 28 -> 32, 1, 7*4, 7*4 -> 32, 1, 7, 7, 4, 4 -> 32, 7, 7, 4, 4, 1 -> 32, 7*7, 4*4*1 - > 32, num_patches, patch_dim\n",
    "        x = rearrange(x, 'b c (nh ph) (nw pw) -> b (nh nw) (ph pw c)', ph=4, pw=4)\n",
    "\n",
    "        # Create patch embedding for all images in the batch\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        #Add position embedding to patch embedding\n",
    "        x = self.position_embedding + x\n",
    "\n",
    "        # concatenating time embedding\n",
    "        x = torch.cat([x, time_proj2[:, :, None, None]], dim=-1)\n",
    "\n",
    "        #Run embedding dropout\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        #Pass patch, position and class embedding through transformer encoder layers (equations 2 & 3)\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # Unpatchify i.e. (B,patches,hidden_size) -> (B,patches,channels * patch_width * patch_height)\n",
    "        x = self.proj_out(x)\n",
    "\n",
    "        # combine all the patches to form image\n",
    "        x = rearrange(x, 'b (nh nw) (ph pw c) -> b c (nh ph) (nw pw)',ph=4,pw=4,nw=7,nh=7)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LcJUd1BfDM9P"
   },
   "outputs": [],
   "source": [
    "model = DIT(\n",
    "            im_size=32,  #128\n",
    "            im_channels=4,  #3\n",
    "            config = {\n",
    "                        'patch_size' : 2,\n",
    "                        'num_layers' : 12,\n",
    "                        'hidden_size' : 768,\n",
    "                        'num_heads' : 12,\n",
    "                        'head_dim' : 64,\n",
    "                        'timestep_emb_dim' : 768\n",
    "                        }\n",
    "            ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCApdvVwCbYM"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wWrn_cc9JGDd"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_samples = 100\n",
    "num_grid_rows = 10\n",
    "\n",
    "model.train()\n",
    "optimizer = Adam(model.parameters(), lr = 0.0001) #optimizer = AdamW(model.parameters(), lr=1E-5, weight_decay=0)\n",
    "\n",
    "betas = torch.linspace(0.0001, 0.02, 1000).to(device) # creating a linear beta schedule for all the timestamps\n",
    "alpha_cum_prod = torch.cumprod(1. - betas, dim=0).to(device) # calculating alpha_bar for each timestamp\n",
    "sqrt_alpha_cum_prod = torch.sqrt(alpha_cum_prod).to(device) # calculating sqrt(alpha_bar) for each timestamp\n",
    "sqrt_one_minus_alpha_cum_prod = torch.sqrt(1 - alpha_cum_prod).to(device) # calculating sqrt(1-alpha_bar) for each timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZOILwbJgDIyq"
   },
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join('celebhq', 'dit_ckpt.pth')):\n",
    "    print('Loaded DiT checkpoint')\n",
    "    model.load_state_dict(torch.load(os.path.join('celebhq', 'dit_ckpt.pth'), map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(40): # running for 40 epochs\n",
    "  losses = []\n",
    "\n",
    "  for im,_ in tqdm(mnist_loader):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    im = im.float().to(device)\n",
    "    noise = torch.randn_like(im).to(device) # sample random noise\n",
    "    t = torch.randint(low = 0, high = 1000, size = (im.shape[0],)).to(device) # sample a random timestamp for each image in the batch\n",
    "    noisy_im = torch.sqrt(alpha_cum_prod[t])[:, None, None, None].to(device) * im + torch.sqrt(1 - alpha_cum_prod[t])[:, None, None, None].to(device) * noise # add noise to image according to the timestamp\n",
    "    noise_pred = model(noisy_im, t) # predicting the added noise\n",
    "\n",
    "    loss = torch.nn.MSELoss()(noise_pred, noise) # loss fucntion\n",
    "    losses.append(loss.item())\n",
    "    loss.backward() # backpropagating the loss\n",
    "    optimizer.step()\n",
    "    \n",
    "  print('Finished epoch:{} | Loss : {:.4f}'.format(epoch_idx + 1,np.mean(losses)))\n",
    "  torch.save(model.state_dict(), os.path.join('celebhq', 'dit_ckpt.pth'))\n",
    "  print()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
