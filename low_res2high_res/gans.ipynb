{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/khetansarvesh/CV/blob/main/low_res2high_res/gans.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAMz9DvKN759",
        "outputId": "ae0467fe-8428-40bc-88f3-dc7a024ec4d5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.22 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import kagglehub\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torchvision.utils import save_image\n",
        "from torchvision.models import vgg19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYhB3189PEUz"
      },
      "outputs": [],
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBs-16qvLvyH"
      },
      "source": [
        "# **Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kP9itt7nZNO",
        "outputId": "5fe33a56-f3b8-4c38-f374-759dd8660ef3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.5)\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/adityachandrasekhar/image-super-resolution?dataset_version_number=2...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 301M/301M [00:16<00:00, 19.1MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/adityachandrasekhar/image-super-resolution/versions/2\n"
          ]
        }
      ],
      "source": [
        "path = kagglehub.dataset_download(\"adityachandrasekhar/image-super-resolution\")\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHbSdBQWLazH"
      },
      "outputs": [],
      "source": [
        "class MyImageFolder(Dataset):\n",
        "    def __init__(self):\n",
        "        super(MyImageFolder, self).__init__()\n",
        "        self.base = \"/root/.cache/kagglehub/datasets/adityachandrasekhar/image-super-resolution/versions/2/dataset/train\"\n",
        "        self.high_images = os.listdir(self.base + '/high_res')\n",
        "        self.low_images = os.listdir(self.base + '/low_res')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.high_images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # finding image index\n",
        "        high_img = self.high_images[index % len(self.high_images)]\n",
        "        low_img = self.low_images[index % len(self.low_images)]\n",
        "\n",
        "        # finding image path\n",
        "        high_path = os.path.join(self.base + \"/high_res\", high_img)\n",
        "        low_path = os.path.join(self.base + \"/low_res\", low_img)\n",
        "\n",
        "        # opening image and storing in array\n",
        "        high_img = np.array(Image.open(high_path).convert(\"RGB\"))\n",
        "        low_img = np.array(Image.open(low_path).convert(\"RGB\"))\n",
        "\n",
        "        # performing transformations on the images zebra and horses\n",
        "        transforms = A.Compose([A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), ToTensorV2()])\n",
        "        high_res = transforms(image=high_img)[\"image\"]\n",
        "        low_res = transforms(image=low_img)[\"image\"]\n",
        "\n",
        "        return low_res, high_res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZN0SizzORq6",
        "outputId": "5e0d14ef-c2ad-4160-bac8-bc72244d6528"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "dataset = MyImageFolder()\n",
        "loader = DataLoader( dataset, batch_size=16, shuffle=True, pin_memory=True, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoQqW1PiMJn9"
      },
      "source": [
        "# **Modelling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3Y6R8Odrs_r"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, img_channels=3, num_features=64, num_residuals=9):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "                                        nn.Conv2d(3, 64, 7, 1, 3, padding_mode=\"reflect\"), nn.InstanceNorm2d(64), nn.ReLU(inplace=True),\n",
        "                                        nn.Conv2d(64, 128, 3, 2, 1, padding_mode=\"reflect\"), nn.InstanceNorm2d(128), nn.ReLU(inplace=True),\n",
        "                                        nn.Conv2d(128, 256, 3, 2, 1, padding_mode=\"reflect\"), nn.InstanceNorm2d(256), nn.ReLU(inplace=True),\n",
        "                                        nn.ConvTranspose2d(256, 128, 3, 2, 1, 1), nn.InstanceNorm2d(128),nn.ReLU(inplace=True),\n",
        "                                        nn.ConvTranspose2d(128, 64, 3, 2, 1, 1), nn.InstanceNorm2d(64),nn.ReLU(inplace=True),\n",
        "                                        nn.Conv2d(64, 3, 7, 1, 3, padding_mode=\"reflect\")\n",
        "                        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.tanh(self.model(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byHyavQmLbYH"
      },
      "outputs": [],
      "source": [
        "# class ConvBlock(nn.Module):\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         in_channels,\n",
        "#         out_channels,\n",
        "#         discriminator=False,\n",
        "#         use_act=True,\n",
        "#         use_bn=True,\n",
        "#         **kwargs,\n",
        "#     ):\n",
        "#         super().__init__()\n",
        "#         self.use_act = use_act\n",
        "#         self.cnn = nn.Conv2d(in_channels, out_channels, **kwargs, bias=not use_bn)\n",
        "#         self.bn = nn.BatchNorm2d(out_channels) if use_bn else nn.Identity()\n",
        "#         self.act = (\n",
        "#             nn.LeakyReLU(0.2, inplace=True)\n",
        "#             if discriminator\n",
        "#             else nn.PReLU(num_parameters=out_channels)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.act(self.bn(self.cnn(x))) if self.use_act else self.bn(self.cnn(x))\n",
        "\n",
        "\n",
        "# class UpsampleBlock(nn.Module):\n",
        "#     def __init__(self, in_c, scale_factor):\n",
        "#         super().__init__()\n",
        "#         self.conv = nn.Conv2d(in_c, in_c * scale_factor ** 2, 3, 1, 1)\n",
        "#         self.ps = nn.PixelShuffle(scale_factor)  # in_c * 4, H, W --> in_c, H*2, W*2\n",
        "#         self.act = nn.PReLU(num_parameters=in_c)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.act(self.ps(self.conv(x)))\n",
        "\n",
        "\n",
        "# class ResidualBlock(nn.Module):\n",
        "#     def __init__(self, in_channels):\n",
        "#         super().__init__()\n",
        "#         self.block1 = ConvBlock(\n",
        "#             in_channels,\n",
        "#             in_channels,\n",
        "#             kernel_size=3,\n",
        "#             stride=1,\n",
        "#             padding=1\n",
        "#         )\n",
        "#         self.block2 = ConvBlock(\n",
        "#             in_channels,\n",
        "#             in_channels,\n",
        "#             kernel_size=3,\n",
        "#             stride=1,\n",
        "#             padding=1,\n",
        "#             use_act=False,\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         out = self.block1(x)\n",
        "#         out = self.block2(out)\n",
        "#         return out + x\n",
        "\n",
        "\n",
        "# class Generator(nn.Module):\n",
        "#     def __init__(self, in_channels=3, num_channels=64, num_blocks=16):\n",
        "#         super().__init__()\n",
        "#         self.initial = ConvBlock(in_channels, num_channels, kernel_size=9, stride=1, padding=4, use_bn=False)\n",
        "#         self.residuals = nn.Sequential(*[ResidualBlock(num_channels) for _ in range(num_blocks)])\n",
        "#         self.convblock = ConvBlock(num_channels, num_channels, kernel_size=3, stride=1, padding=1, use_act=False)\n",
        "#         self.upsamples = nn.Sequential(UpsampleBlock(num_channels, 2), UpsampleBlock(num_channels, 2))\n",
        "#         self.final = nn.Conv2d(num_channels, in_channels, kernel_size=9, stride=1, padding=4)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         initial = self.initial(x)\n",
        "#         x = self.residuals(initial)\n",
        "#         x = self.convblock(x) + initial\n",
        "#         x = self.upsamples(x)\n",
        "#         return torch.tanh(self.final(x))\n",
        "\n",
        "\n",
        "# class Discriminator(nn.Module):\n",
        "#     def __init__(self, in_channels=3, features=[64, 64, 128, 128, 256, 256, 512, 512]):\n",
        "#         super().__init__()\n",
        "#         blocks = []\n",
        "#         for idx, feature in enumerate(features):\n",
        "#             blocks.append(\n",
        "#                 ConvBlock(\n",
        "#                     in_channels,\n",
        "#                     feature,\n",
        "#                     kernel_size=3,\n",
        "#                     stride=1 + idx % 2,\n",
        "#                     padding=1,\n",
        "#                     discriminator=True,\n",
        "#                     use_act=True,\n",
        "#                     use_bn=False if idx == 0 else True,\n",
        "#                 )\n",
        "#             )\n",
        "#             in_channels = feature\n",
        "\n",
        "#         self.blocks = nn.Sequential(*blocks)\n",
        "#         self.classifier = nn.Sequential(\n",
        "#             nn.AdaptiveAvgPool2d((6, 6)),\n",
        "#             nn.Flatten(),\n",
        "#             nn.Linear(512*6*6, 1024),\n",
        "#             nn.LeakyReLU(0.2, inplace=True),\n",
        "#             nn.Linear(1024, 1),\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.blocks(x)\n",
        "#         return self.classifier(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Rdp-IlzrxPk"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "                                    nn.Conv2d(3, 64, 4, 2, 1, padding_mode=\"reflect\"), nn.LeakyReLU(0.2, inplace=True),\n",
        "                                    nn.Conv2d(64, 128, 4, 2, 1, bias = True, padding_mode=\"reflect\"), nn.InstanceNorm2d(128), nn.LeakyReLU(0.2, inplace=True),\n",
        "                                    nn.Conv2d(128, 256, 4, 2, 1, bias = True, padding_mode=\"reflect\"), nn.InstanceNorm2d(256), nn.LeakyReLU(0.2, inplace=True),\n",
        "                                    nn.Conv2d(256, 512, 4, 1, 1, bias = True, padding_mode=\"reflect\"), nn.InstanceNorm2d(512), nn.LeakyReLU(0.2, inplace=True),\n",
        "                                    nn.Conv2d(512, 1, 4, 1, 1, padding_mode=\"reflect\")\n",
        "                                  )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(self.model(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB-X5VoKL08S"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgVXrcHmOgBp"
      },
      "outputs": [],
      "source": [
        "# instantiate the model\n",
        "gen = Generator().to(DEVICE)\n",
        "disc = Discriminator().to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeCDT3O1OaR8"
      },
      "outputs": [],
      "source": [
        "# Optimizers\n",
        "opt_gen = optim.Adam(gen.parameters(), lr=1e-4, betas=(0.9, 0.999))\n",
        "opt_disc = optim.Adam(disc.parameters(), lr=1e-4, betas=(0.9, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXZG6K6kOXei",
        "outputId": "18588284-ae1c-4900-ddbd-47a7c0a42a5f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:03<00:00, 154MB/s]\n"
          ]
        }
      ],
      "source": [
        "# losses\n",
        "class VGGLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.vgg = vgg19(pretrained=True).features[:36].eval().to(DEVICE)\n",
        "        self.loss = nn.MSELoss()\n",
        "\n",
        "        for param in self.vgg.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        vgg_input_features = self.vgg(input)\n",
        "        vgg_target_features = self.vgg(target)\n",
        "        return self.loss(vgg_input_features, vgg_target_features)\n",
        "\n",
        "\n",
        "mse = nn.MSELoss()\n",
        "bce = nn.BCEWithLogitsLoss()\n",
        "vgg_loss = VGGLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O90IevivLbVv",
        "outputId": "d875ee9b-4c5d-4ef0-8b59-7e873260b43f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 43/43 [01:46<00:00,  2.47s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.05s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.05s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.05s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.07s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.05s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.05s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.05s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.07s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.07s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.05s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.05s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.05s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.05s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.05s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.06s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.06s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.05s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.05s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.06s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.05s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.06s/it]\n",
            "100%|██████████| 43/43 [00:46<00:00,  1.07s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.07s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.06s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.06s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.06s/it]\n",
            "100%|██████████| 43/43 [00:46<00:00,  1.07s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.07s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.06s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.06s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.06s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.06s/it]\n",
            "100%|██████████| 43/43 [00:46<00:00,  1.07s/it]\n",
            "100%|██████████| 43/43 [00:46<00:00,  1.08s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.06s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.06s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.06s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.06s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.07s/it]\n",
            "100%|██████████| 43/43 [00:46<00:00,  1.07s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.06s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.06s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.06s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.06s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.06s/it]\n",
            "100%|██████████| 43/43 [00:46<00:00,  1.07s/it]\n",
            "100%|██████████| 43/43 [00:45<00:00,  1.06s/it]\n",
            " 56%|█████▌    | 24/43 [00:25<00:20,  1.06s/it]"
          ]
        }
      ],
      "source": [
        "for epoch in range(100):\n",
        "    loop = tqdm(loader, leave=True)\n",
        "\n",
        "    for idx, (low_res, high_res) in enumerate(loop):\n",
        "\n",
        "        high_res = high_res.to(DEVICE)\n",
        "        low_res = low_res.to(DEVICE)\n",
        "\n",
        "\n",
        "\n",
        "        ''' Training Discriminator first keeping Generator Constant '''\n",
        "        fake = gen(low_res)\n",
        "        disc_real = disc(high_res)\n",
        "        disc_fake = disc(fake.detach())\n",
        "        disc_loss_real = bce(disc_real, torch.ones_like(disc_real) - 0.1 * torch.rand_like(disc_real))\n",
        "        disc_loss_fake = bce(disc_fake, torch.zeros_like(disc_fake))\n",
        "        loss_disc = disc_loss_fake + disc_loss_real\n",
        "\n",
        "        opt_disc.zero_grad()\n",
        "        loss_disc.backward()\n",
        "        opt_disc.step()\n",
        "\n",
        "        '''Training Generator Next Keeping Discriminator Constant'''\n",
        "        disc_fake = disc(fake)\n",
        "        adversarial_loss = 1e-3 * bce(disc_fake, torch.ones_like(disc_fake))\n",
        "        loss_for_vgg = 0.006 * vgg_loss(fake, high_res)\n",
        "        gen_loss = loss_for_vgg + adversarial_loss\n",
        "\n",
        "        opt_gen.zero_grad()\n",
        "        gen_loss.backward()\n",
        "        opt_gen.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPTIRmW41yoy"
      },
      "source": [
        "# ****Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMgqUIn-sEue"
      },
      "outputs": [],
      "source": [
        "gen.eval()\n",
        "image = Image.open(\"test_images/\" + file)\n",
        "with torch.no_grad():\n",
        "    test_transform = A.Compose([A.Normalize(mean=[0, 0, 0], std=[1, 1, 1]),ToTensorV2()])\n",
        "    transformed_img = test_transform(image=np.asarray(image))[\"image\"]\n",
        "    upscaled_img = gen(transformed_img.unsqueeze(0).to(DEVICE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ib0t83n4yYTk"
      },
      "outputs": [],
      "source": [
        "upscaled_img"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "oBs-16qvLvyH"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
