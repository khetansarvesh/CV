{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ap-5uzVEXb2"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/khetansarvesh/CV/blob/main/object_detection/yolo_version1/runner.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "DoFjl99L4JG1"
   },
   "outputs": [],
   "source": [
    "# !pip install tqdm==4.66.4\n",
    "# !pip install torchvision==0.18.1\n",
    "# !pip install torch==2.3.1\n",
    "# !pip install albumentations==1.4.13\n",
    "# !pip install Pillow==10.4.0\n",
    "# !pip install opencv_python==4.10.0.84\n",
    "# !pip install einops==0.8.0\n",
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "g8lyaXja9cvk"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import yaml\n",
    "import csv\n",
    "import os\n",
    "import albumentations as albu\n",
    "import cv2\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn as nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "d8f4Qy469taK"
   },
   "outputs": [],
   "source": [
    "seed = 1111\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wvvHVr0WEc-U",
    "outputId": "8cfbd3d8-7ac5-4448-bb5d-1b5a37870c12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'CV'...\n",
      "remote: Enumerating objects: 841, done.\u001b[K\n",
      "remote: Counting objects: 100% (306/306), done.\u001b[K\n",
      "remote: Compressing objects: 100% (141/141), done.\u001b[K\n",
      "remote: Total 841 (delta 158), reused 279 (delta 140), pack-reused 535 (from 1)\u001b[K\n",
      "Receiving objects: 100% (841/841), 28.54 MiB | 25.93 MiB/s, done.\n",
      "Resolving deltas: 100% (455/455), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/khetansarvesh/CV.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqbKXAjk9Uwk"
   },
   "source": [
    "# **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gA6RDy5i4JG3",
    "outputId": "07b04f57-a072-4d77-c6aa-54fc74238c55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-05 19:51:12--  http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
      "Resolving host.robots.ox.ac.uk (host.robots.ox.ac.uk)... 129.67.94.152\n",
      "Connecting to host.robots.ox.ac.uk (host.robots.ox.ac.uk)|129.67.94.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 460032000 (439M) [application/x-tar]\n",
      "Saving to: ‘VOCtrainval_06-Nov-2007.tar’\n",
      "\n",
      "VOCtrainval_06-Nov- 100%[===================>] 438.72M  41.3MB/s    in 12s     \n",
      "\n",
      "2024-11-05 19:51:24 (36.6 MB/s) - ‘VOCtrainval_06-Nov-2007.tar’ saved [460032000/460032000]\n",
      "\n",
      "--2024-11-05 19:51:26--  http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Resolving host.robots.ox.ac.uk (host.robots.ox.ac.uk)... 129.67.94.152\n",
      "Connecting to host.robots.ox.ac.uk (host.robots.ox.ac.uk)|129.67.94.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1999639040 (1.9G) [application/x-tar]\n",
      "Saving to: ‘VOCtrainval_11-May-2012.tar’\n",
      "\n",
      "VOCtrainval_11-May- 100%[===================>]   1.86G  40.7MB/s    in 58s     \n",
      "\n",
      "2024-11-05 19:52:25 (32.6 MB/s) - ‘VOCtrainval_11-May-2012.tar’ saved [1999639040/1999639040]\n",
      "\n",
      "--2024-11-05 19:52:39--  http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n",
      "Resolving host.robots.ox.ac.uk (host.robots.ox.ac.uk)... 129.67.94.152\n",
      "Connecting to host.robots.ox.ac.uk (host.robots.ox.ac.uk)|129.67.94.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 451020800 (430M) [application/x-tar]\n",
      "Saving to: ‘VOCtest_06-Nov-2007.tar’\n",
      "\n",
      "VOCtest_06-Nov-2007 100%[===================>] 430.13M  33.9MB/s    in 14s     \n",
      "\n",
      "2024-11-05 19:52:53 (31.5 MB/s) - ‘VOCtest_06-Nov-2007.tar’ saved [451020800/451020800]\n",
      "\n",
      "--2024-11-05 19:52:55--  https://pjreddie.com/media/files/voc_label.py\n",
      "Resolving pjreddie.com (pjreddie.com)... 162.0.215.52\n",
      "Connecting to pjreddie.com (pjreddie.com)|162.0.215.52|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2042 (2.0K) [text/x-python]\n",
      "Saving to: ‘voc_label.py’\n",
      "\n",
      "voc_label.py        100%[===================>]   1.99K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-11-05 19:52:56 (697 MB/s) - ‘voc_label.py’ saved [2042/2042]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GETTING VOC2007 TRAIN DATASET and EXTRACTING TAR FILES\n",
    "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
    "!tar xf VOCtrainval_06-Nov-2007.tar\n",
    "\n",
    "# GETTING VOC2012 TRAIN DATASET and EXTRACTING TAR FILES\n",
    "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
    "!tar xf VOCtrainval_11-May-2012.tar\n",
    "\n",
    "# GETTING VOC2007 TEST DATASET and EXTRACTING TAR FILES\n",
    "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar #\n",
    "!tar xf VOCtest_06-Nov-2007.tar\n",
    "\n",
    "## Gettting the images location for 2007 (both train and test) and 2012 (only train) dataset in txt files\n",
    "# !wget https://pjreddie.com/media/files/voc_label.py\n",
    "# !python voc_label.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exEJhP394JG4"
   },
   "outputs": [],
   "source": [
    "def load_images_and_anns(label2idx, ann_fname, split):\n",
    "    r\"\"\"\n",
    "    Method to get the xml files and for each file get all the objects and their ground truth detection information for the dataset\n",
    "    \"\"\"\n",
    "    im_infos = []\n",
    "    ims = []\n",
    "    im_sets = ['VOCdevkit/VOC2007', 'VOCdevkit/VOC2012'] #since we will be using VOC dataset\n",
    "\n",
    "    for im_set in im_sets:\n",
    "        im_names = []\n",
    "\n",
    "        # Fetch all image names in txt file for this imageset\n",
    "        for line in open(os.path.join(im_set, 'ImageSets', 'Main', f'{ann_fname}.txt')):\n",
    "            im_names.append(line.strip())\n",
    "\n",
    "        # Set annotation (or label) and image path\n",
    "        ann_dir = os.path.join(im_set, 'Annotations')\n",
    "        im_dir = os.path.join(im_set, 'JPEGImages')\n",
    "\n",
    "        # iterate over all the images\n",
    "        for im_name in im_names:\n",
    "\n",
    "            # getting some meta data for that image\n",
    "            ann_file = os.path.join(ann_dir, f'{im_name}.xml')\n",
    "            im_info = {}\n",
    "            ann_info = ET.parse(ann_file)\n",
    "            root = ann_info.getroot()\n",
    "            size = root.find('size')\n",
    "            width = int(size.find('width').text)\n",
    "            height = int(size.find('height').text)\n",
    "            im_info['img_id'] = os.path.basename(ann_file).split('.xml')[0]\n",
    "            im_info['filename'] = os.path.join(im_dir, '{}.jpg'.format(im_info['img_id']))\n",
    "            im_info['width'] = width\n",
    "            im_info['height'] = height\n",
    "            detections = []\n",
    "\n",
    "            # We will keep an image only if there are valid rois in it\n",
    "            any_valid_object = False\n",
    "\n",
    "            # getting all the rois for this image\n",
    "            for obj in ann_info.findall('object'):\n",
    "                det = {}\n",
    "                label = label2idx[obj.find('name').text]\n",
    "                difficult = int(obj.find('difficult').text)\n",
    "                bbox_info = obj.find('bndbox')\n",
    "                bbox = [\n",
    "                    int(float(bbox_info.find('xmin').text))-1,\n",
    "                    int(float(bbox_info.find('ymin').text))-1,\n",
    "                    int(float(bbox_info.find('xmax').text))-1,\n",
    "                    int(float(bbox_info.find('ymax').text))-1\n",
    "                ]\n",
    "                det['label'] = label\n",
    "                det['bbox'] = bbox\n",
    "                det['difficult'] = difficult\n",
    "                # Ignore difficult rois during training\n",
    "                # At test time eval does the job of ignoring difficult\n",
    "                # examples.\n",
    "                if difficult == 0 or split == 'test':\n",
    "                    detections.append(det)\n",
    "                    any_valid_object = True\n",
    "\n",
    "            if any_valid_object:\n",
    "                im_info['detections'] = detections\n",
    "                im_infos.append(im_info)\n",
    "\n",
    "    print(f'Total {len(im_infos)} images found')\n",
    "    print(f'Example Information : {im_infos[0]}')\n",
    "    return im_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ox53ZAWa4JG4"
   },
   "outputs": [],
   "source": [
    "class VOCDataset(Dataset):\n",
    "    def __init__(self, split):\n",
    "\n",
    "        # to decide if we are working with training data or testing data\n",
    "        self.split = split\n",
    "        self.fname = 'trainval' if self.split == 'train' else 'test'\n",
    "\n",
    "        self.im_size = 448 # img size is 448*448\n",
    "        self.S = 7 # 7*7 grid size\n",
    "        self.B = 2 # no of bounding box prediction per grid\n",
    "\n",
    "        self.C = 20 # no of classes\n",
    "        classes = ['person', 'bird', 'cat', 'cow', 'dog', 'horse', 'sheep',\n",
    "            'aeroplane', 'bicycle', 'boat', 'bus', 'car', 'motorbike', 'train',\n",
    "            'bottle', 'chair', 'diningtable', 'pottedplant', 'sofa', 'tvmonitor'] # defining all the 20 classes present in the VOC dataset\n",
    "        classes = sorted(classes)\n",
    "\n",
    "        # creating a dictionary to map class name to index eg 'person' : 0\n",
    "        self.label2idx = {classes[idx]: idx for idx in range(len(classes))}\n",
    "\n",
    "        # creating a vice versa dictionary i.e. 0 : 'person'\n",
    "        self.idx2label = {idx: classes[idx] for idx in range(len(classes))}\n",
    "\n",
    "        # getting the VOC Dataset images\n",
    "        self.images_info = load_images_and_anns(self.label2idx, self.fname, self.split)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_info)\n",
    "    \n",
    "    '''Here in this method we will convert bounding boxes from absoulute coordinates to grid \n",
    "    relative coordinates'''\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # getting the image\n",
    "        im_info = self.images_info[index]\n",
    "        im = cv2.imread(im_info['filename'])\n",
    "        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Get label and bounding box information for this image\n",
    "        bboxes = [detection['bbox'] for detection in im_info['detections']]\n",
    "        labels = [detection['label'] for detection in im_info['detections']]\n",
    "        difficult = [detection['difficult'] for detection in im_info['detections']]\n",
    "\n",
    "        # Transform Image and label => hence you will also have to transform bounding boxes\n",
    "        transforms = {\n",
    "\n",
    "            # performing transformations on train dataset\n",
    "            'train': albu.Compose([\n",
    "                albu.HorizontalFlip(p=0.5),\n",
    "                albu.Affine(\n",
    "                    scale=(0.8, 1.2),\n",
    "                    translate_percent=(-0.2, 0.2),\n",
    "                    always_apply=True\n",
    "                ),\n",
    "                albu.ColorJitter(\n",
    "                    brightness=(0.8, 1.2),\n",
    "                    contrast=(0.8, 1.2),\n",
    "                    saturation=(0.8, 1.2),\n",
    "                    hue=(-0.2, 0.2),\n",
    "                    always_apply=None,\n",
    "                    p=0.5,\n",
    "                ),\n",
    "                albu.Resize(self.im_size, self.im_size)],\n",
    "                bbox_params=albu.BboxParams(format='pascal_voc',\n",
    "                                            label_fields=['labels'])),\n",
    "\n",
    "\n",
    "            # performing transformations on test datset\n",
    "            'test': albu.Compose([\n",
    "                albu.Resize(self.im_size, self.im_size),\n",
    "                ],\n",
    "                bbox_params=albu.BboxParams(format='pascal_voc',\n",
    "                                            label_fields=['labels']))\n",
    "        }\n",
    "        transformed_info = transforms[self.split](image=im, bboxes=bboxes, labels=labels)\n",
    "        im = transformed_info['image']\n",
    "        bboxes = torch.as_tensor(transformed_info['bboxes'])\n",
    "        labels = torch.as_tensor(transformed_info['labels'])\n",
    "        difficult = torch.as_tensor(difficult)\n",
    "\n",
    "        # Convert image to tensor and normalize (since we will use resnet backbone, it expects input in this format)\n",
    "        im_tensor = torch.from_numpy(im / 255.).permute((2, 0, 1)).float()\n",
    "        im_tensor_channel_0 = (torch.unsqueeze(im_tensor[0], 0) - 0.485) / 0.229\n",
    "        im_tensor_channel_1 = (torch.unsqueeze(im_tensor[1], 0) - 0.456) / 0.224\n",
    "        im_tensor_channel_2 = (torch.unsqueeze(im_tensor[2], 0) - 0.406) / 0.225\n",
    "        im_tensor = torch.cat((im_tensor_channel_0,\n",
    "                               im_tensor_channel_1,\n",
    "                               im_tensor_channel_2), 0)\n",
    "        bboxes_tensor = torch.as_tensor(bboxes)\n",
    "        labels_tensor = torch.as_tensor(labels)\n",
    "\n",
    "        '''Build Target for Yolo'''\n",
    "\n",
    "        target_dim = 5 * self.B + self.C\n",
    "        h, w = im.shape[:2]\n",
    "        yolo_targets = torch.zeros(self.S, self.S, target_dim)\n",
    "\n",
    "        # Height and width of grid cells is H // S\n",
    "        cell_pixels = h // self.S\n",
    "\n",
    "        if len(bboxes) > 0:\n",
    "\n",
    "            # VOC dataset has (x1, y1, x2, y2) format of bounding box so we need to convert it into (Xcenter, Ycenter, width, heigh) format\n",
    "            box_widths = bboxes_tensor[:, 2] - bboxes_tensor[:, 0]\n",
    "            box_heights = bboxes_tensor[:, 3] - bboxes_tensor[:, 1]\n",
    "            box_center_x = bboxes_tensor[:, 0] + 0.5 * box_widths\n",
    "            box_center_y = bboxes_tensor[:, 1] + 0.5 * box_heights\n",
    "\n",
    "            # Get cell i,j from xc, yc\n",
    "            box_i = torch.floor(box_center_x / cell_pixels).long()\n",
    "            box_j = torch.floor(box_center_y / cell_pixels).long()\n",
    "\n",
    "            # Xcenter offset from cell topleft\n",
    "            box_xc_cell_offset = (box_center_x - box_i*cell_pixels) / cell_pixels\n",
    "            box_yc_cell_offset = (box_center_y - box_j*cell_pixels) / cell_pixels\n",
    "\n",
    "            # w, h targets normalized to 0-1\n",
    "            box_w_label = box_widths / w\n",
    "            box_h_label = box_heights / h\n",
    "\n",
    "            # Update the target array for all bboxes\n",
    "            for idx, b in enumerate(range(bboxes_tensor.size(0))):\n",
    "                # Make target of the exact same shape as prediction\n",
    "                for k in range(self.B):\n",
    "                    s = 5 * k\n",
    "                    # target_ij = [xc_offset,yc_offset,sqrt(w),sqrt(h), conf, cls_label]\n",
    "                    yolo_targets[box_j[idx], box_i[idx], s] = box_xc_cell_offset[idx]\n",
    "                    yolo_targets[box_j[idx], box_i[idx], s+1] = box_yc_cell_offset[idx]\n",
    "                    yolo_targets[box_j[idx], box_i[idx], s+2] = box_w_label[idx].sqrt()\n",
    "                    yolo_targets[box_j[idx], box_i[idx], s+3] = box_h_label[idx].sqrt()\n",
    "                    yolo_targets[box_j[idx], box_i[idx], s+4] = 1.0\n",
    "                label = int(labels[b])\n",
    "                cls_target = torch.zeros((self.C,))\n",
    "                cls_target[label] = 1.\n",
    "                yolo_targets[box_j[idx], box_i[idx], 5 * self.B:] = cls_target\n",
    "        # For training, we use yolo_targets(xoffset, yoffset, sqrt(w), sqrt(h))\n",
    "        # For evaluation we use bboxes_tensor (x1, y1, x2, y2)\n",
    "        # Below we normalize bboxes tensor to be between 0-1\n",
    "        # as thats what evaluation script expects so (x1/w, y1/h, x2/w, y2/h)\n",
    "        if len(bboxes) > 0:\n",
    "            bboxes_tensor /= torch.Tensor([[w, h, w, h]]).expand_as(bboxes_tensor)\n",
    "        targets = {\n",
    "            'bboxes': bboxes_tensor,\n",
    "            'labels': labels_tensor,\n",
    "            'yolo_targets': yolo_targets,\n",
    "            'difficult': difficult,\n",
    "        }\n",
    "        return im_tensor, targets, im_info['filename']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tz7wkgcU9SSC",
    "outputId": "4f13e07e-abf0-4b93-aacc-b3a1cc8f16dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 16551 images found\n",
      "Example Information : {'img_id': '000005', 'filename': 'VOCdevkit/VOC2007/JPEGImages/000005.jpg', 'width': 500, 'height': 375, 'detections': [{'label': 8, 'bbox': [262, 210, 323, 338], 'difficult': 0}, {'label': 8, 'bbox': [164, 263, 252, 371], 'difficult': 0}, {'label': 8, 'bbox': [240, 193, 294, 298], 'difficult': 0}]}\n"
     ]
    }
   ],
   "source": [
    "def collate_function(data):\n",
    "    return list(zip(*data))\n",
    "\n",
    "voc = VOCDataset('train')\n",
    "train_dataset = DataLoader(voc, batch_size=64, shuffle=True, collate_fn = collate_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nK5pM4US9hCi"
   },
   "source": [
    "# **Modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "XATLzq6j_c7Q"
   },
   "outputs": [],
   "source": [
    "class YOLOV1(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(YOLOV1, self).__init__()\n",
    "\n",
    "        backbone = torchvision.models.resnet34(weights=torchvision.models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "        self.features = nn.Sequential(\n",
    "            backbone.conv1,\n",
    "            backbone.bn1,\n",
    "            backbone.relu,\n",
    "            backbone.maxpool,\n",
    "            backbone.layer1,\n",
    "            backbone.layer2,\n",
    "            backbone.layer3,\n",
    "            backbone.layer4,\n",
    "        )\n",
    "\n",
    "\n",
    "        self.conv_yolo_layers = nn.Sequential(\n",
    "            # convolution layer 1\n",
    "            nn.Conv2d(512, 1024, 3, padding=1, bias=False), #512 cause resnet gives 512 channel output\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.1),\n",
    "\n",
    "            # convolution layer 2\n",
    "            nn.Conv2d(1024, 1024, 3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.1),\n",
    "\n",
    "            # convolution layer 3\n",
    "            nn.Conv2d(1024, 1024, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.1),\n",
    "\n",
    "            # convolution layer 4\n",
    "            nn.Conv2d(1024, 1024, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.1)\n",
    "            )\n",
    "\n",
    "        self.fc_yolo_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(7 * 7 * 1024, 4096),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 7 * 7 * (5 * 2 + 20)),\n",
    "        )\n",
    "        # instead of this you could have also used a 1*1 convolution layer as follows\n",
    "        # self.fc_yolo_layers = nn.Sequential( nn.Conv2d(1024, 5 * self.B + self.C, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = self.conv_yolo_layers(out)\n",
    "        out = self.fc_yolo_layers(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ab714p1x9iGu",
    "outputId": "7a221d4c-a72a-4278-ccfd-e6ff267866db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLOV1(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_yolo_layers): Sequential(\n",
       "    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.1)\n",
       "    (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): LeakyReLU(negative_slope=0.1)\n",
       "    (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): LeakyReLU(negative_slope=0.1)\n",
       "    (9): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (10): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): LeakyReLU(negative_slope=0.1)\n",
       "  )\n",
       "  (fc_yolo_layers): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=50176, out_features=4096, bias=True)\n",
       "    (2): LeakyReLU(negative_slope=0.1)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=1470, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yolo_model = YOLOV1().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dzSSurx9WIT"
   },
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "GjlnswVcFNYO"
   },
   "outputs": [],
   "source": [
    "# set the model to training setting\n",
    "yolo_model.train()\n",
    "\n",
    "# defining optimizer\n",
    "optimizer = torch.optim.SGD(lr=0.001,\n",
    "                            params=filter(lambda p: p.requires_grad,yolo_model.parameters()),\n",
    "                            weight_decay=5E-4,\n",
    "                            momentum=0.9)\n",
    "\n",
    "# defining scheduler\n",
    "scheduler = MultiStepLR(optimizer, milestones=[ 50, 75, 100, 125 ], gamma=0.5)\n",
    "\n",
    "\n",
    "# defining loss function\n",
    "from CV.object_detection.yolo_version1.loss import YOLOV1Loss\n",
    "criterion = YOLOV1Loss()\n",
    "\n",
    "\n",
    "# other training parameters\n",
    "acc_steps = 1\n",
    "num_epochs = 135\n",
    "steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "qhzGFRTb9Wqy",
    "outputId": "9af137c8-a9b8-49a1-cd1a-2bba15fcfdc3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/259 [03:03<13:09:50, 183.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 19.0473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 1/259 [03:58<17:04:33, 238.27s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-9dbf273b7d5b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m             for target in targets], dim=0)\n\u001b[1;32m      8\u001b[0m         \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mims\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0myolo_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myolo_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myolo_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myolo_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_sigmoid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0macc_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-5c3e596a908a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_yolo_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_yolo_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative_slope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mleaky_relu\u001b[0;34m(input, negative_slope, inplace)\u001b[0m\n\u001b[1;32m   1900\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1901\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1902\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1903\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch_idx in range(num_epochs):\n",
    "    losses = []\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for idx, (ims, targets, _) in enumerate(tqdm(train_dataset)):\n",
    "        yolo_targets = torch.cat([\n",
    "            target['yolo_targets'].unsqueeze(0).float().to(device)\n",
    "            for target in targets], dim=0)\n",
    "        im = torch.cat([im.unsqueeze(0).float().to(device) for im in ims], dim=0)\n",
    "        yolo_preds = yolo_model(im)\n",
    "        loss = criterion(yolo_preds, yolo_targets)\n",
    "        loss = loss / acc_steps\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if (idx + 1) % acc_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if steps % 100 == 0:\n",
    "            print('Loss : {:.4f}'.format(np.mean(losses)))\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            print('Loss is becoming nan. Exiting')\n",
    "            exit(0)\n",
    "        steps += 1\n",
    "\n",
    "    print('Finished epoch {}'.format(epoch_idx+1))\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    scheduler.step()\n",
    "    #torch.save(yolo_model.state_dict(), os.path.join(train_config['task_name'], train_config['ckpt_name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39jLK65H9XA5"
   },
   "source": [
    "# **Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vFnwpoMBu07"
   },
   "outputs": [],
   "source": [
    "dataset_config = {\n",
    "                    'test_im_sets': ['data/VOC2007-test'],\n",
    "                    'num_classes' : 20,\n",
    "                    'im_size' : 448\n",
    "                    }\n",
    "\n",
    "train_config = {\n",
    "                'task_name': 'voc',\n",
    "                'acc_steps': 1, # increase if you want to get gradients from >1 steps(kind of mimicking >1 batch size)\n",
    "                'log_steps': 100,\n",
    "                'num_epochs': 135,\n",
    "                'batch_size': 64,\n",
    "                'lr_steps': ,\n",
    "                'lr': 0.001,\n",
    "                'infer_conf_threshold' : 0.2,\n",
    "                'eval_conf_threshold' : 0.001,\n",
    "                'nms_threshold' : 0.5,\n",
    "                'ckpt_name': 'yolo_voc2007.pth'\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oGSmE1FV9YxX"
   },
   "outputs": [],
   "source": [
    "from CV.object_detection.yolo_version1.infer import infer, evaluate_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pR7CzaxFFnEJ"
   },
   "outputs": [],
   "source": [
    "infer(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OlYjShK0GBBx"
   },
   "outputs": [],
   "source": [
    "evaluate_map(args)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "TqbKXAjk9Uwk",
    "nK5pM4US9hCi",
    "2dzSSurx9WIT",
    "39jLK65H9XA5"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
