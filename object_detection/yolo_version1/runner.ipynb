{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ap-5uzVEXb2"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/khetansarvesh/CV/blob/main/object_detection/yolo_version1/runner.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tqdm==4.66.4\n",
    "# !pip install torchvision==0.18.1\n",
    "# !pip install torch==2.3.1\n",
    "# !pip install albumentations==1.4.13\n",
    "# !pip install Pillow==10.4.0\n",
    "# !pip install opencv_python==4.10.0.84\n",
    "# !pip install einops==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "g8lyaXja9cvk"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np \n",
    "import yaml\n",
    "import csv\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import torch.nn as nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bENy4HHw-viz"
   },
   "outputs": [],
   "source": [
    "dataset_config = {\n",
    "                    'test_im_sets': ['data/VOC2007-test'],\n",
    "                    'num_classes' : 20,\n",
    "                    'im_size' : 448\n",
    "                    }\n",
    "\n",
    "train_config = {\n",
    "                'task_name': 'voc',\n",
    "                'seed': 1111,\n",
    "                'acc_steps': 1, # increase if you want to get gradients from >1 steps(kind of mimicking >1 batch size)\n",
    "                'log_steps': 100,\n",
    "                'num_epochs': 135,\n",
    "                'batch_size': 64,\n",
    "                'lr_steps': [ 50, 75, 100, 125 ],\n",
    "                'lr': 0.001,\n",
    "                'infer_conf_threshold' : 0.2,\n",
    "                'eval_conf_threshold' : 0.001,\n",
    "                'nms_threshold' : 0.5,\n",
    "                'ckpt_name': 'yolo_voc2007.pth'\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "d8f4Qy469taK"
   },
   "outputs": [],
   "source": [
    "seed = train_config['seed']\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wvvHVr0WEc-U",
    "outputId": "22cdbaaf-a1af-4f05-afd1-d0d6c3880768"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'CV'...\n",
      "remote: Enumerating objects: 820, done.\u001b[K\n",
      "remote: Counting objects: 100% (285/285), done.\u001b[K\n",
      "remote: Compressing objects: 100% (124/124), done.\u001b[K\n",
      "remote: Total 820 (delta 144), reused 274 (delta 136), pack-reused 535 (from 1)\u001b[K\n",
      "Receiving objects: 100% (820/820), 28.52 MiB | 46.14 MiB/s, done.\n",
      "Resolving deltas: 100% (441/441), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/khetansarvesh/CV.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqbKXAjk9Uwk"
   },
   "source": [
    "# **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-05 17:04:56--  http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
      "Resolving host.robots.ox.ac.uk (host.robots.ox.ac.uk)... 129.67.94.152\n",
      "Connecting to host.robots.ox.ac.uk (host.robots.ox.ac.uk)|129.67.94.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 460032000 (439M) [application/x-tar]\n",
      "Saving to: ‘VOCtrainval_06-Nov-2007.tar’\n",
      "\n",
      "VOCtrainval_06-Nov- 100%[===================>] 438.72M  35.9MB/s    in 14s     \n",
      "\n",
      "2024-11-05 17:05:09 (32.2 MB/s) - ‘VOCtrainval_06-Nov-2007.tar’ saved [460032000/460032000]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GETTING VOC2007 TRAIN DATASET and EXTRACTING TAR FILES                                                             \n",
    "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
    "!tar xf VOCtrainval_06-Nov-2007.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-05 17:05:17--  http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Resolving host.robots.ox.ac.uk (host.robots.ox.ac.uk)... 129.67.94.152\n",
      "Connecting to host.robots.ox.ac.uk (host.robots.ox.ac.uk)|129.67.94.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1999639040 (1.9G) [application/x-tar]\n",
      "Saving to: ‘VOCtrainval_11-May-2012.tar’\n",
      "\n",
      "VOCtrainval_11-May- 100%[===================>]   1.86G  33.9MB/s    in 55s     \n",
      "\n",
      "2024-11-05 17:06:13 (34.6 MB/s) - ‘VOCtrainval_11-May-2012.tar’ saved [1999639040/1999639040]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GETTING VOC2012 TRAIN DATASET and EXTRACTING TAR FILES                                                               \n",
    "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
    "!tar xf VOCtrainval_11-May-2012.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING VOC2007 TEST DATASET and EXTRACTING TAR FILES                                                             \n",
    "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar # \n",
    "!tar xf VOCtest_06-Nov-2007.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gettting the images location for 2007 (both train and test) and 2012 (only train) dataset in txt files\n",
    "!wget https://pjreddie.com/media/files/voc_label.py\n",
    "!python voc_label.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training we will use train dataset from both 2007 and 2012 and hence we concatenate them and store in a new file called train.txt\n",
    "!cat 2007_train.txt 2007_val.txt 2012_*.txt > train.txt\n",
    "\n",
    "# For testing we will only use test from 2007 and we store that in test.txt\n",
    "!cp 2007_test.txt test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move txt files we won't be using to clean up a little bit\n",
    "mkdir old_txt_files\n",
    "mv 2007* 2012* old_txt_files/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_train = open(\"train.txt\", \"r\").readlines()\n",
    "\n",
    "with open(\"train.csv\", mode=\"w\", newline=\"\") as train_file:\n",
    "    for line in read_train:\n",
    "        image_file = line.split(\"/\")[-1].replace(\"\\n\", \"\")\n",
    "        text_file = image_file.replace(\".jpg\", \".txt\")\n",
    "        data = [image_file, text_file]\n",
    "        writer = csv.writer(train_file)\n",
    "        writer.writerow(data)\n",
    "\n",
    "read_train = open(\"test.txt\", \"r\").readlines()\n",
    "\n",
    "with open(\"test.csv\", mode=\"w\", newline=\"\") as train_file:\n",
    "    for line in read_train:\n",
    "        image_file = line.split(\"/\")[-1].replace(\"\\n\", \"\")\n",
    "        text_file = image_file.replace(\".jpg\", \".txt\")\n",
    "        data = [image_file, text_file]\n",
    "        writer = csv.writer(train_file)\n",
    "        writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir data\n",
    "mkdir data/images\n",
    "mkdir data/labels\n",
    "\n",
    "cp VOCdevkit/*.jpg data/images/\n",
    "cp VOCdevkit/VOC2007/labels/*.txt data/labels/\n",
    "cp VOCdevkit/VOC2012/labels/*.txt data/labels/\n",
    "\n",
    "mkdir data                                                                              \n",
    "mkdir data/images                                                                       \n",
    "mkdir data/labels                                                                       \n",
    "                                                                                        \n",
    "mv VOCdevkit/VOC2007/JPEGImages/*.jpg data/images/                                      \n",
    "mv VOCdevkit/VOC2012/JPEGImages/*.jpg data/images/                                      \n",
    "mv VOCdevkit/VOC2007/labels/*.txt data/labels/                                          \n",
    "mv VOCdevkit/VOC2012/labels/*.txt data/labels/ \n",
    "\n",
    "# We don't need VOCdevkit folder anymore, can remove\n",
    "# in order to save some space \n",
    "rm -rf VOCdevkit/\n",
    "mv test.txt old_txt_files/\n",
    "mv train.txt old_txt_files/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riigy5NRE-KW"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libGL.so.1: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# from CV.object_detection.yolo_version1.dataset import VOCDataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VOCDataset\n\u001b[1;32m      3\u001b[0m voc \u001b[38;5;241m=\u001b[39m VOCDataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/workspaces/CV/object_detection/yolo_version1/dataset.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01malbumentations\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01malbu\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/albumentations/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01malbumentations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheck_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_for_updates\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugmentations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserialization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/albumentations/augmentations/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblur\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblur\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcrops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/albumentations/augmentations/blur/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/albumentations/augmentations/blur/functional.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ceil\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal, Sequence\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01malbucore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clipped, maybe_process_in_chunks, preserve_channel_dim\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/cv2/__init__.py:181\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m DEBUG: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra Python code for\u001b[39m\u001b[38;5;124m\"\u001b[39m, submodule, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis loaded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m DEBUG: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpenCV loader: DONE\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbootstrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/cv2/__init__.py:153\u001b[0m, in \u001b[0;36mbootstrap\u001b[0;34m()\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DEBUG: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRelink everything from native cv2 module to cv2 package\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    151\u001b[0m py_module \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 153\u001b[0m native_module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcv2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv2\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m py_module\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28msetattr\u001b[39m(py_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_native\u001b[39m\u001b[38;5;124m\"\u001b[39m, native_module)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mImportError\u001b[0m: libGL.so.1: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "from CV.object_detection.yolo_version1.dataset import VOCDataset\n",
    "voc = VOCDataset('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tz7wkgcU9SSC"
   },
   "outputs": [],
   "source": [
    "def collate_function(data):\n",
    "    return list(zip(*data))\n",
    "\n",
    "train_dataset = DataLoader(voc, batch_size=train_config['batch_size'], shuffle=True, collate_fn=collate_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nK5pM4US9hCi"
   },
   "source": [
    "# **Modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XATLzq6j_c7Q"
   },
   "outputs": [],
   "source": [
    "class YOLOV1(nn.Module):\n",
    "\n",
    "    def __init__(self, im_size, num_classes, model_config):\n",
    "        super(YOLOV1, self).__init__()\n",
    "        self.im_size = im_size\n",
    "        self.im_channels = model_config['im_channels']\n",
    "        self.backbone_channels = model_config['backbone_channels']\n",
    "        self.yolo_conv_channels = model_config['yolo_conv_channels']\n",
    "        self.conv_spatial_size = model_config['conv_spatial_size']\n",
    "        self.leaky_relu_slope = model_config['leaky_relu_slope']\n",
    "        self.yolo_fc_hidden_dim = model_config['fc_dim']\n",
    "        self.yolo_fc_dropout_prob = model_config['fc_dropout']\n",
    "        self.use_conv = model_config['use_conv']\n",
    "        self.S = model_config['S']\n",
    "        self.B = model_config['B']\n",
    "        self.C = num_classes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ###################\n",
    "        # Backbone Layers # resnet34 pretrained on 224x224 images from Imagenet\n",
    "        ###################\n",
    "        backbone = torchvision.models.resnet34(weights=torchvision.models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            backbone.conv1,\n",
    "            backbone.bn1,\n",
    "            backbone.relu,\n",
    "            backbone.maxpool,\n",
    "            backbone.layer1,\n",
    "            backbone.layer2,\n",
    "            backbone.layer3,\n",
    "            backbone.layer4,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #########################\n",
    "        # Detection Conv Layers # 4 Conv,Batchnorm,LeakyReLU Layers for Yolo Detection Head\n",
    "        #########################\n",
    "        self.conv_yolo_layers = nn.Sequential(\n",
    "            nn.Conv2d(self.backbone_channels,\n",
    "                      self.yolo_conv_channels,\n",
    "                      3,\n",
    "                      padding=1,\n",
    "                      bias=False),\n",
    "            nn.BatchNorm2d(self.yolo_conv_channels),\n",
    "            nn.LeakyReLU(self.leaky_relu_slope),\n",
    "            nn.Conv2d(self.yolo_conv_channels,\n",
    "                      self.yolo_conv_channels,\n",
    "                      3,\n",
    "                      stride=2,\n",
    "                      padding=1,\n",
    "                      bias=False),\n",
    "            nn.BatchNorm2d(self.yolo_conv_channels),\n",
    "            nn.LeakyReLU(self.leaky_relu_slope),\n",
    "            nn.Conv2d(self.yolo_conv_channels,\n",
    "                      self.yolo_conv_channels,\n",
    "                      3,\n",
    "                      padding=1,\n",
    "                      bias=False),\n",
    "            nn.BatchNorm2d(self.yolo_conv_channels),\n",
    "            nn.LeakyReLU(self.leaky_relu_slope),\n",
    "            nn.Conv2d(self.yolo_conv_channels,\n",
    "                      self.yolo_conv_channels,\n",
    "                      3,\n",
    "                      padding=1,\n",
    "                      bias=False),\n",
    "            nn.BatchNorm2d(self.yolo_conv_channels),\n",
    "            nn.LeakyReLU(self.leaky_relu_slope)\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #######################\n",
    "        # Detection Layers #\n",
    "        '''\n",
    "        Fc layers with final layer having S*S*(5B+C) output dimensions\n",
    "        Final layer predicts [\n",
    "            x_offset_box1,y_offset_box1,sqrt_w_box1,sqrt_h_box1,conf_box1, # box-1 params\n",
    "            ...,\n",
    "            x_offset_boxB,y_offset_boxB,sqrt_w_boxB,sqrt_h_boxB,conf_boxB, # box-B params\n",
    "            p1, p2, ...., pC-1, pC  # class conditional probabilities\n",
    "        ] for each S*S grid cell\n",
    "        '''\n",
    "        #######################\n",
    "        if self.use_conv:\n",
    "            self.fc_yolo_layers = nn.Sequential(\n",
    "                nn.Conv2d(self.yolo_conv_channels, 5 * self.B + self.C, 1),\n",
    "            )\n",
    "        else:\n",
    "            self.fc_yolo_layers = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(self.conv_spatial_size * self.conv_spatial_size *\n",
    "                          self.yolo_conv_channels,\n",
    "                          self.yolo_fc_hidden_dim),\n",
    "                nn.LeakyReLU(self.leaky_relu_slope),\n",
    "                nn.Dropout(self.yolo_fc_dropout_prob),\n",
    "                nn.Linear(self.yolo_fc_hidden_dim,\n",
    "                          self.S * self.S * (5 * self.B + self.C)),\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = self.conv_yolo_layers(out)\n",
    "        out = self.fc_yolo_layers(out)\n",
    "        if self.use_conv:\n",
    "            # Reshape conv output to Batch x S x S x (5B+C)\n",
    "            out = out.permute(0, 2, 3, 1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ab714p1x9iGu"
   },
   "outputs": [],
   "source": [
    "model_config = {\n",
    "                'im_channels' : 3,\n",
    "                'backbone_channels' : 512,\n",
    "                'conv_spatial_size' : 7,\n",
    "                'yolo_conv_channels' : 1024,\n",
    "                'leaky_relu_slope' : 0.1,\n",
    "                'fc_dim' : 4096,\n",
    "                'fc_dropout' : 0.5,\n",
    "                'S' : 7,\n",
    "                'B' : 2,\n",
    "                'use_sigmoid' : True,\n",
    "                'use_conv' : True\n",
    "                }\n",
    "\n",
    "yolo_model = YOLOV1(im_size=dataset_config['im_size'],\n",
    "                    num_classes=dataset_config['num_classes'],\n",
    "                    model_config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dzSSurx9WIT"
   },
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GjlnswVcFNYO"
   },
   "outputs": [],
   "source": [
    "yolo_model.train()\n",
    "yolo_model.to(device)\n",
    "if os.path.exists(os.path.join(train_config['task_name'],\n",
    "                                train_config['ckpt_name'])):\n",
    "    print('Loading checkpoint as one exists')\n",
    "    yolo_model.load_state_dict(torch.load(\n",
    "        os.path.join(train_config['task_name'],\n",
    "                        train_config['ckpt_name']),\n",
    "        map_location=device))\n",
    "if not os.path.exists(train_config['task_name']):\n",
    "    os.mkdir(train_config['task_name'])\n",
    "\n",
    "optimizer = torch.optim.SGD(lr=train_config['lr'],\n",
    "                            params=filter(lambda p: p.requires_grad,\n",
    "                                            yolo_model.parameters()),\n",
    "                            weight_decay=5E-4,\n",
    "                            momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QyzIS6gY-CK4"
   },
   "outputs": [],
   "source": [
    "scheduler = MultiStepLR(optimizer, milestones=train_config['lr_steps'], gamma=0.5)\n",
    "\n",
    "from CV.object_detection.yolo_version1.loss import YOLOV1Loss\n",
    "criterion = YOLOV1Loss()\n",
    "\n",
    "acc_steps = train_config['acc_steps']\n",
    "num_epochs = train_config['num_epochs']\n",
    "steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qhzGFRTb9Wqy"
   },
   "outputs": [],
   "source": [
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    losses = []\n",
    "    optimizer.zero_grad()\n",
    "    for idx, (ims, targets, _) in enumerate(tqdm(train_dataset)):\n",
    "        yolo_targets = torch.cat([\n",
    "            target['yolo_targets'].unsqueeze(0).float().to(device)\n",
    "            for target in targets], dim=0)\n",
    "        im = torch.cat([im.unsqueeze(0).float().to(device) for im in ims], dim=0)\n",
    "        yolo_preds = yolo_model(im)\n",
    "        loss = criterion(yolo_preds, yolo_targets, use_sigmoid=model_config['use_sigmoid'])\n",
    "        loss = loss / acc_steps\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        if (idx + 1) % acc_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        if steps % train_config['log_steps'] == 0:\n",
    "            print('Loss : {:.4f}'.format(np.mean(losses)))\n",
    "        if torch.isnan(loss):\n",
    "            print('Loss is becoming nan. Exiting')\n",
    "            exit(0)\n",
    "        steps += 1\n",
    "    print('Finished epoch {}'.format(epoch_idx+1))\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    scheduler.step()\n",
    "    torch.save(yolo_model.state_dict(), os.path.join(train_config['task_name'],\n",
    "                                                        train_config['ckpt_name']))\n",
    "print('Done Training...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39jLK65H9XA5"
   },
   "source": [
    "# **Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oGSmE1FV9YxX"
   },
   "outputs": [],
   "source": [
    "from CV.object_detection.yolo_version1.infer import infer, evaluate_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pR7CzaxFFnEJ"
   },
   "outputs": [],
   "source": [
    "infer(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OlYjShK0GBBx"
   },
   "outputs": [],
   "source": [
    "evaluate_map(args)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "TqbKXAjk9Uwk",
    "nK5pM4US9hCi",
    "2dzSSurx9WIT",
    "39jLK65H9XA5"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
