{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ap-5uzVEXb2"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/khetansarvesh/CV/blob/main/object_detection/yolo_version1/runner.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python-headless in /usr/local/python/3.12.1/lib/python3.12/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /home/codespace/.local/lib/python3.12/site-packages (from opencv-python-headless) (2.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install tqdm==4.66.4\n",
    "# !pip install torchvision==0.18.1\n",
    "# !pip install torch==2.3.1\n",
    "# !pip install albumentations==1.4.13\n",
    "# !pip install Pillow==10.4.0\n",
    "# !pip install opencv_python==4.10.0.84\n",
    "# !pip install einops==0.8.0\n",
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g8lyaXja9cvk"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libGL.so.1: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# import albumentations as albu\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01metree\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mElementTree\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mET\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: libGL.so.1: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np \n",
    "import yaml\n",
    "import csv\n",
    "import os\n",
    "import albumentations as albu\n",
    "import cv2\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn as nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bENy4HHw-viz"
   },
   "outputs": [],
   "source": [
    "dataset_config = {\n",
    "                    'test_im_sets': ['data/VOC2007-test'],\n",
    "                    'num_classes' : 20,\n",
    "                    'im_size' : 448\n",
    "                    }\n",
    "\n",
    "train_config = {\n",
    "                'task_name': 'voc',\n",
    "                'seed': 1111,\n",
    "                'acc_steps': 1, # increase if you want to get gradients from >1 steps(kind of mimicking >1 batch size)\n",
    "                'log_steps': 100,\n",
    "                'num_epochs': 135,\n",
    "                'batch_size': 64,\n",
    "                'lr_steps': [ 50, 75, 100, 125 ],\n",
    "                'lr': 0.001,\n",
    "                'infer_conf_threshold' : 0.2,\n",
    "                'eval_conf_threshold' : 0.001,\n",
    "                'nms_threshold' : 0.5,\n",
    "                'ckpt_name': 'yolo_voc2007.pth'\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "d8f4Qy469taK"
   },
   "outputs": [],
   "source": [
    "seed = train_config['seed']\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wvvHVr0WEc-U",
    "outputId": "22cdbaaf-a1af-4f05-afd1-d0d6c3880768"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'CV'...\n",
      "remote: Enumerating objects: 820, done.\u001b[K\n",
      "remote: Counting objects: 100% (285/285), done.\u001b[K\n",
      "remote: Compressing objects: 100% (124/124), done.\u001b[K\n",
      "remote: Total 820 (delta 144), reused 274 (delta 136), pack-reused 535 (from 1)\u001b[K\n",
      "Receiving objects: 100% (820/820), 28.52 MiB | 46.14 MiB/s, done.\n",
      "Resolving deltas: 100% (441/441), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/khetansarvesh/CV.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqbKXAjk9Uwk"
   },
   "source": [
    "# **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-05 19:16:46--  http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
      "Resolving host.robots.ox.ac.uk (host.robots.ox.ac.uk)... 129.67.94.152\n",
      "Connecting to host.robots.ox.ac.uk (host.robots.ox.ac.uk)|129.67.94.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 460032000 (439M) [application/x-tar]\n",
      "Saving to: ‘VOCtrainval_06-Nov-2007.tar’\n",
      "\n",
      "VOCtrainval_06-Nov- 100%[===================>] 438.72M  25.2MB/s    in 25s     \n",
      "\n",
      "2024-11-05 19:17:11 (17.8 MB/s) - ‘VOCtrainval_06-Nov-2007.tar’ saved [460032000/460032000]\n",
      "\n",
      "--2024-11-05 19:17:15--  http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
      "Resolving host.robots.ox.ac.uk (host.robots.ox.ac.uk)... 129.67.94.152\n",
      "Connecting to host.robots.ox.ac.uk (host.robots.ox.ac.uk)|129.67.94.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1999639040 (1.9G) [application/x-tar]\n",
      "Saving to: ‘VOCtrainval_11-May-2012.tar’\n",
      "\n",
      "VOCtrainval_11-May- 100%[===================>]   1.86G  36.8MB/s    in 58s     \n",
      "\n",
      "2024-11-05 19:18:14 (32.7 MB/s) - ‘VOCtrainval_11-May-2012.tar’ saved [1999639040/1999639040]\n",
      "\n",
      "--2024-11-05 19:18:50--  http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n",
      "Resolving host.robots.ox.ac.uk (host.robots.ox.ac.uk)... 129.67.94.152\n",
      "Connecting to host.robots.ox.ac.uk (host.robots.ox.ac.uk)|129.67.94.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 451020800 (430M) [application/x-tar]\n",
      "Saving to: ‘VOCtest_06-Nov-2007.tar’\n",
      "\n",
      "VOCtest_06-Nov-2007 100%[===================>] 430.13M  6.71MB/s    in 66s     \n",
      "\n",
      "2024-11-05 19:19:56 (6.49 MB/s) - ‘VOCtest_06-Nov-2007.tar’ saved [451020800/451020800]\n",
      "\n",
      "--2024-11-05 19:19:58--  https://pjreddie.com/media/files/voc_label.py\n",
      "Resolving pjreddie.com (pjreddie.com)... 162.0.215.52\n",
      "Connecting to pjreddie.com (pjreddie.com)|162.0.215.52|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2042 (2.0K) [text/x-python]\n",
      "Saving to: ‘voc_label.py’\n",
      "\n",
      "voc_label.py        100%[===================>]   1.99K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-11-05 19:20:00 (25.0 MB/s) - ‘voc_label.py’ saved [2042/2042]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GETTING VOC2007 TRAIN DATASET and EXTRACTING TAR FILES                                                             \n",
    "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
    "!tar xf VOCtrainval_06-Nov-2007.tar\n",
    "\n",
    "# GETTING VOC2012 TRAIN DATASET and EXTRACTING TAR FILES                                                               \n",
    "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
    "!tar xf VOCtrainval_11-May-2012.tar\n",
    "\n",
    "# GETTING VOC2007 TEST DATASET and EXTRACTING TAR FILES                                                             \n",
    "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar # \n",
    "!tar xf VOCtest_06-Nov-2007.tar\n",
    "\n",
    "## Gettting the images location for 2007 (both train and test) and 2012 (only train) dataset in txt files\n",
    "!wget https://pjreddie.com/media/files/voc_label.py\n",
    "!python voc_label.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For training we will use train dataset from both 2007 and 2012 and hence we concatenate them and store in a new file called train.txt\n",
    "# !cat 2007_train.txt 2007_val.txt 2012_*.txt > train.txt\n",
    "\n",
    "# # creating training csv using train.txt\n",
    "# read_train = open(\"train.txt\", \"r\").readlines()\n",
    "# with open(\"train.csv\", mode=\"w\", newline=\"\") as train_file:\n",
    "#     for line in read_train:\n",
    "#         image_file = line.split(\"/\")[-1].replace(\"\\n\", \"\")\n",
    "#         text_file = image_file.replace(\".jpg\", \".txt\")\n",
    "#         data = [image_file, text_file]\n",
    "#         writer = csv.writer(train_file)\n",
    "#         writer.writerow(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # For testing we will only use test from 2007 and we store that in test.txt\n",
    "# !cp 2007_test.txt test.txt\n",
    "\n",
    "# # creating testing csv using test.txt\n",
    "# read_train = open(\"test.txt\", \"r\").readlines()\n",
    "# with open(\"test.csv\", mode=\"w\", newline=\"\") as train_file:\n",
    "#     for line in read_train:\n",
    "#         image_file = line.split(\"/\")[-1].replace(\"\\n\", \"\")\n",
    "#         text_file = image_file.replace(\".jpg\", \".txt\")\n",
    "#         data = [image_file, text_file]\n",
    "#         writer = csv.writer(train_file)\n",
    "#         writer.writerow(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Move txt files we won't be using to clean up a little bit\n",
    "# !mkdir old_txt_files\n",
    "# !mv 2007* 2012* old_txt_files/\n",
    "# !mv test.txt old_txt_files/\n",
    "# !mv train.txt old_txt_files/\n",
    "# !mv VOCtest_06-Nov-2007.tar old_txt_files/\n",
    "# !mv VOCtrainval_06-Nov-2007.tar old_txt_files/\n",
    "# !mv VOCtrainval_11-May-2012.tar old_txt_files/\n",
    "\n",
    "# # We don't need files in old_txt_files anymore, so deleting it\n",
    "# !rm -rf old_txt_files/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# !mkdir data                                                                              \n",
    "# !mkdir data/images                                                                       \n",
    "# !mkdir data/labels                                                                       \n",
    "                                                                                        \n",
    "# !mv VOCdevkit/VOC2007/JPEGImages/*.jpg data/images/                                      \n",
    "# !mv VOCdevkit/VOC2012/JPEGImages/*.jpg data/images/                                      \n",
    "# !mv VOCdevkit/VOC2007/labels/*.txt data/labels/                                          \n",
    "# !mv VOCdevkit/VOC2012/labels/*.txt data/labels/ \n",
    "\n",
    "# # We don't need VOCdevkit folder anymore, can remove it in order to save some space \n",
    "# !rm -rf VOCdevkit/\n",
    "# !rm voc_label.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_and_anns(label2idx, ann_fname, split):\n",
    "    r\"\"\"\n",
    "    Method to get the xml files and for each file\n",
    "    get all the objects and their ground truth detection\n",
    "    information for the dataset\n",
    "    :param label2idx: Class Name to index mapping for dataset\n",
    "    :param ann_fname: txt file containing image names{trainval.txt/test.txt}\n",
    "    :param split: train/test\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    im_infos = []\n",
    "    ims = []\n",
    "    im_sets = ['VOCdevkit/VOC2007', 'VOCdevkit/VOC2012'] #since we will be using VOC dataset\n",
    "\n",
    "    for im_set in im_sets:\n",
    "        im_names = []\n",
    "        # Fetch all image names in txt file for this imageset\n",
    "        for line in open(os.path.join(im_set, 'ImageSets', 'Main', f'{ann_fname}.txt')):\n",
    "            im_names.append(line.strip())\n",
    "\n",
    "        # Set annotation and image path\n",
    "        ann_dir = os.path.join(im_set, 'Annotations')\n",
    "        im_dir = os.path.join(im_set, 'JPEGImages')\n",
    "        for im_name in im_names:\n",
    "            ann_file = os.path.join(ann_dir, f'{im_name}.xml')\n",
    "            im_info = {}\n",
    "            ann_info = ET.parse(ann_file)\n",
    "            root = ann_info.getroot()\n",
    "            size = root.find('size')\n",
    "            width = int(size.find('width').text)\n",
    "            height = int(size.find('height').text)\n",
    "            im_info['img_id'] = os.path.basename(ann_file).split('.xml')[0]\n",
    "            im_info['filename'] = os.path.join(im_dir, '{}.jpg'.format(im_info['img_id']))\n",
    "            im_info['width'] = width\n",
    "            im_info['height'] = height\n",
    "            detections = []\n",
    "\n",
    "            # We will keep an image only if there are valid rois in it\n",
    "            any_valid_object = False\n",
    "            for obj in ann_info.findall('object'):\n",
    "                det = {}\n",
    "                label = label2idx[obj.find('name').text]\n",
    "                difficult = int(obj.find('difficult').text)\n",
    "                bbox_info = obj.find('bndbox')\n",
    "                bbox = [\n",
    "                    int(float(bbox_info.find('xmin').text))-1,\n",
    "                    int(float(bbox_info.find('ymin').text))-1,\n",
    "                    int(float(bbox_info.find('xmax').text))-1,\n",
    "                    int(float(bbox_info.find('ymax').text))-1\n",
    "                ]\n",
    "                det['label'] = label\n",
    "                det['bbox'] = bbox\n",
    "                det['difficult'] = difficult\n",
    "                # Ignore difficult rois during training\n",
    "                # At test time eval does the job of ignoring difficult\n",
    "                # examples. \n",
    "                if difficult == 0 or split == 'test':\n",
    "                    detections.append(det)\n",
    "                    any_valid_object = True\n",
    "\n",
    "            if any_valid_object:\n",
    "                im_info['detections'] = detections\n",
    "                im_infos.append(im_info)\n",
    "    print('Total {} images found'.format(len(im_infos)))\n",
    "    print(im_infos[0])\n",
    "    return im_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDataset(Dataset):\n",
    "    def __init__(self, split):\n",
    "\n",
    "        # to decide if we are working with training data or testing data\n",
    "        self.split = split\n",
    "        self.fname = 'trainval' if self.split == 'train' else 'test'\n",
    "\n",
    "        self.im_size = 448 # img size is 448*448\n",
    "        self.S = 7 # 7*7 grid size\n",
    "        self.B = 2 # no of bounding box prediction per grid\n",
    "\n",
    "        self.C = 20 # no of classes\n",
    "        classes = ['person', 'bird', 'cat', 'cow', 'dog', 'horse', 'sheep',\n",
    "            'aeroplane', 'bicycle', 'boat', 'bus', 'car', 'motorbike', 'train',\n",
    "            'bottle', 'chair', 'diningtable', 'pottedplant', 'sofa', 'tvmonitor'] # defining all the 20 classes present in the VOC dataset\n",
    "        classes = sorted(classes)\n",
    "\n",
    "        # creating a dictionary to map class name to index eg 'person' : 0\n",
    "        self.label2idx = {classes[idx]: idx for idx in range(len(classes))}\n",
    "\n",
    "        # creating a vice versa dictionary i.e. 0 : 'person'\n",
    "        self.idx2label = {idx: classes[idx] for idx in range(len(classes))}\n",
    "\n",
    "        # getting the VOC Dataset images\n",
    "        self.images_info = load_images_and_anns(self.label2idx, self.fname, self.split)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images_info)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        im_info = self.images_info[index]\n",
    "        im = cv2.imread(im_info['filename'])\n",
    "        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Get annotations for this image\n",
    "        bboxes = [detection['bbox'] for detection in im_info['detections']]\n",
    "        labels = [detection['label'] for detection in im_info['detections']]\n",
    "        difficult = [detection['difficult'] for detection in im_info['detections']]\n",
    "\n",
    "        # Transform Image and ann according to augmentations list\n",
    "        transforms = {\n",
    "\n",
    "            # performing transformations on train dataset\n",
    "            'train': albu.Compose([\n",
    "                albu.HorizontalFlip(p=0.5),\n",
    "                albu.Affine(\n",
    "                    scale=(0.8, 1.2),\n",
    "                    translate_percent=(-0.2, 0.2),\n",
    "                    always_apply=True\n",
    "                ),\n",
    "                albu.ColorJitter(\n",
    "                    brightness=(0.8, 1.2),\n",
    "                    contrast=(0.8, 1.2),\n",
    "                    saturation=(0.8, 1.2),\n",
    "                    hue=(-0.2, 0.2),\n",
    "                    always_apply=None,\n",
    "                    p=0.5,\n",
    "                ),\n",
    "                albu.Resize(self.im_size, self.im_size)],\n",
    "                bbox_params=albu.BboxParams(format='pascal_voc',\n",
    "                                            label_fields=['labels'])),\n",
    "\n",
    "\n",
    "            # performing transformations on test datset\n",
    "            'test': albu.Compose([\n",
    "                albu.Resize(self.im_size, self.im_size),\n",
    "                ],\n",
    "                bbox_params=albu.BboxParams(format='pascal_voc',\n",
    "                                            label_fields=['labels']))\n",
    "        }\n",
    "        transformed_info = transforms[self.split](image=im, bboxes=bboxes, labels=labels)\n",
    "        im = transformed_info['image']\n",
    "        bboxes = torch.as_tensor(transformed_info['bboxes'])\n",
    "        labels = torch.as_tensor(transformed_info['labels'])\n",
    "        difficult = torch.as_tensor(difficult)\n",
    "\n",
    "        # Convert image to tensor and normalize\n",
    "        im_tensor = torch.from_numpy(im / 255.).permute((2, 0, 1)).float()\n",
    "        im_tensor_channel_0 = (torch.unsqueeze(im_tensor[0], 0) - 0.485) / 0.229\n",
    "        im_tensor_channel_1 = (torch.unsqueeze(im_tensor[1], 0) - 0.456) / 0.224\n",
    "        im_tensor_channel_2 = (torch.unsqueeze(im_tensor[2], 0) - 0.406) / 0.225\n",
    "        im_tensor = torch.cat((im_tensor_channel_0,\n",
    "                               im_tensor_channel_1,\n",
    "                               im_tensor_channel_2), 0)\n",
    "        bboxes_tensor = torch.as_tensor(bboxes)\n",
    "        labels_tensor = torch.as_tensor(labels)\n",
    "\n",
    "        # Build Target for Yolo\n",
    "        target_dim = 5 * self.B + self.C\n",
    "        h, w = im.shape[:2]\n",
    "        yolo_targets = torch.zeros(self.S, self.S, target_dim)\n",
    "\n",
    "        # Height and width of grid cells is H // S\n",
    "        cell_pixels = h // self.S\n",
    "\n",
    "        if len(bboxes) > 0:\n",
    "            # Convert x1y1x2y2 to xywh format\n",
    "            box_widths = bboxes_tensor[:, 2] - bboxes_tensor[:, 0]\n",
    "            box_heights = bboxes_tensor[:, 3] - bboxes_tensor[:, 1]\n",
    "            box_center_x = bboxes_tensor[:, 0] + 0.5 * box_widths\n",
    "            box_center_y = bboxes_tensor[:, 1] + 0.5 * box_heights\n",
    "\n",
    "            # Get cell i,j from xc, yc\n",
    "            box_i = torch.floor(box_center_x / cell_pixels).long()\n",
    "            box_j = torch.floor(box_center_y / cell_pixels).long()\n",
    "\n",
    "            # xc offset from cell topleft\n",
    "            box_xc_cell_offset = (box_center_x - box_i*cell_pixels) / cell_pixels\n",
    "            box_yc_cell_offset = (box_center_y - box_j*cell_pixels) / cell_pixels\n",
    "\n",
    "            # w, h targets normalized to 0-1\n",
    "            box_w_label = box_widths / w\n",
    "            box_h_label = box_heights / h\n",
    "\n",
    "            # Update the target array for all bboxes\n",
    "            for idx, b in enumerate(range(bboxes_tensor.size(0))):\n",
    "                # Make target of the exact same shape as prediction\n",
    "                for k in range(self.B):\n",
    "                    s = 5 * k\n",
    "                    # target_ij = [xc_offset,yc_offset,sqrt(w),sqrt(h), conf, cls_label]\n",
    "                    yolo_targets[box_j[idx], box_i[idx], s] = box_xc_cell_offset[idx]\n",
    "                    yolo_targets[box_j[idx], box_i[idx], s+1] = box_yc_cell_offset[idx]\n",
    "                    yolo_targets[box_j[idx], box_i[idx], s+2] = box_w_label[idx].sqrt()\n",
    "                    yolo_targets[box_j[idx], box_i[idx], s+3] = box_h_label[idx].sqrt()\n",
    "                    yolo_targets[box_j[idx], box_i[idx], s+4] = 1.0\n",
    "                label = int(labels[b])\n",
    "                cls_target = torch.zeros((self.C,))\n",
    "                cls_target[label] = 1.\n",
    "                yolo_targets[box_j[idx], box_i[idx], 5 * self.B:] = cls_target\n",
    "        # For training, we use yolo_targets(xoffset, yoffset, sqrt(w), sqrt(h))\n",
    "        # For evaluation we use bboxes_tensor (x1, y1, x2, y2)\n",
    "        # Below we normalize bboxes tensor to be between 0-1\n",
    "        # as thats what evaluation script expects so (x1/w, y1/h, x2/w, y2/h)\n",
    "        if len(bboxes) > 0:\n",
    "            bboxes_tensor /= torch.Tensor([[w, h, w, h]]).expand_as(bboxes_tensor)\n",
    "        targets = {\n",
    "            'bboxes': bboxes_tensor,\n",
    "            'labels': labels_tensor,\n",
    "            'yolo_targets': yolo_targets,\n",
    "            'difficult': difficult,\n",
    "        }\n",
    "        return im_tensor, targets, im_info['filename']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tz7wkgcU9SSC"
   },
   "outputs": [],
   "source": [
    "def collate_function(data):\n",
    "    return list(zip(*data))\n",
    "\n",
    "voc = VOCDataset('train')\n",
    "train_dataset = DataLoader(voc, batch_size=train_config['batch_size'], shuffle=True, collate_fn = collate_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nK5pM4US9hCi"
   },
   "source": [
    "# **Modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XATLzq6j_c7Q"
   },
   "outputs": [],
   "source": [
    "class YOLOV1(nn.Module):\n",
    "\n",
    "    def __init__(self, im_size, num_classes, model_config):\n",
    "        super(YOLOV1, self).__init__()\n",
    "        self.im_size = im_size\n",
    "        self.im_channels = model_config['im_channels']\n",
    "        self.backbone_channels = model_config['backbone_channels']\n",
    "        self.yolo_conv_channels = model_config['yolo_conv_channels']\n",
    "        self.conv_spatial_size = model_config['conv_spatial_size']\n",
    "        self.leaky_relu_slope = model_config['leaky_relu_slope']\n",
    "        self.yolo_fc_hidden_dim = model_config['fc_dim']\n",
    "        self.yolo_fc_dropout_prob = model_config['fc_dropout']\n",
    "        self.use_conv = model_config['use_conv']\n",
    "        self.S = model_config['S']\n",
    "        self.B = model_config['B']\n",
    "        self.C = num_classes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ###################\n",
    "        # Backbone Layers # resnet34 pretrained on 224x224 images from Imagenet\n",
    "        ###################\n",
    "        backbone = torchvision.models.resnet34(weights=torchvision.models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            backbone.conv1,\n",
    "            backbone.bn1,\n",
    "            backbone.relu,\n",
    "            backbone.maxpool,\n",
    "            backbone.layer1,\n",
    "            backbone.layer2,\n",
    "            backbone.layer3,\n",
    "            backbone.layer4,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #########################\n",
    "        # Detection Conv Layers # 4 Conv,Batchnorm,LeakyReLU Layers for Yolo Detection Head\n",
    "        #########################\n",
    "        self.conv_yolo_layers = nn.Sequential(\n",
    "            nn.Conv2d(self.backbone_channels,\n",
    "                      self.yolo_conv_channels,\n",
    "                      3,\n",
    "                      padding=1,\n",
    "                      bias=False),\n",
    "            nn.BatchNorm2d(self.yolo_conv_channels),\n",
    "            nn.LeakyReLU(self.leaky_relu_slope),\n",
    "            nn.Conv2d(self.yolo_conv_channels,\n",
    "                      self.yolo_conv_channels,\n",
    "                      3,\n",
    "                      stride=2,\n",
    "                      padding=1,\n",
    "                      bias=False),\n",
    "            nn.BatchNorm2d(self.yolo_conv_channels),\n",
    "            nn.LeakyReLU(self.leaky_relu_slope),\n",
    "            nn.Conv2d(self.yolo_conv_channels,\n",
    "                      self.yolo_conv_channels,\n",
    "                      3,\n",
    "                      padding=1,\n",
    "                      bias=False),\n",
    "            nn.BatchNorm2d(self.yolo_conv_channels),\n",
    "            nn.LeakyReLU(self.leaky_relu_slope),\n",
    "            nn.Conv2d(self.yolo_conv_channels,\n",
    "                      self.yolo_conv_channels,\n",
    "                      3,\n",
    "                      padding=1,\n",
    "                      bias=False),\n",
    "            nn.BatchNorm2d(self.yolo_conv_channels),\n",
    "            nn.LeakyReLU(self.leaky_relu_slope)\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #######################\n",
    "        # Detection Layers #\n",
    "        '''\n",
    "        Fc layers with final layer having S*S*(5B+C) output dimensions\n",
    "        Final layer predicts [\n",
    "            x_offset_box1,y_offset_box1,sqrt_w_box1,sqrt_h_box1,conf_box1, # box-1 params\n",
    "            ...,\n",
    "            x_offset_boxB,y_offset_boxB,sqrt_w_boxB,sqrt_h_boxB,conf_boxB, # box-B params\n",
    "            p1, p2, ...., pC-1, pC  # class conditional probabilities\n",
    "        ] for each S*S grid cell\n",
    "        '''\n",
    "        #######################\n",
    "        if self.use_conv:\n",
    "            self.fc_yolo_layers = nn.Sequential(\n",
    "                nn.Conv2d(self.yolo_conv_channels, 5 * self.B + self.C, 1),\n",
    "            )\n",
    "        else:\n",
    "            self.fc_yolo_layers = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(self.conv_spatial_size * self.conv_spatial_size *\n",
    "                          self.yolo_conv_channels,\n",
    "                          self.yolo_fc_hidden_dim),\n",
    "                nn.LeakyReLU(self.leaky_relu_slope),\n",
    "                nn.Dropout(self.yolo_fc_dropout_prob),\n",
    "                nn.Linear(self.yolo_fc_hidden_dim,\n",
    "                          self.S * self.S * (5 * self.B + self.C)),\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = self.conv_yolo_layers(out)\n",
    "        out = self.fc_yolo_layers(out)\n",
    "        if self.use_conv:\n",
    "            # Reshape conv output to Batch x S x S x (5B+C)\n",
    "            out = out.permute(0, 2, 3, 1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ab714p1x9iGu"
   },
   "outputs": [],
   "source": [
    "model_config = {\n",
    "                'im_channels' : 3,\n",
    "                'backbone_channels' : 512,\n",
    "                'conv_spatial_size' : 7,\n",
    "                'yolo_conv_channels' : 1024,\n",
    "                'leaky_relu_slope' : 0.1,\n",
    "                'fc_dim' : 4096,\n",
    "                'fc_dropout' : 0.5,\n",
    "                'S' : 7,\n",
    "                'B' : 2,\n",
    "                'use_sigmoid' : True,\n",
    "                'use_conv' : True\n",
    "                }\n",
    "\n",
    "yolo_model = YOLOV1(im_size=dataset_config['im_size'],\n",
    "                    num_classes=dataset_config['num_classes'],\n",
    "                    model_config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dzSSurx9WIT"
   },
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GjlnswVcFNYO"
   },
   "outputs": [],
   "source": [
    "yolo_model.train()\n",
    "yolo_model.to(device)\n",
    "if os.path.exists(os.path.join(train_config['task_name'],\n",
    "                                train_config['ckpt_name'])):\n",
    "    print('Loading checkpoint as one exists')\n",
    "    yolo_model.load_state_dict(torch.load(\n",
    "        os.path.join(train_config['task_name'],\n",
    "                        train_config['ckpt_name']),\n",
    "        map_location=device))\n",
    "if not os.path.exists(train_config['task_name']):\n",
    "    os.mkdir(train_config['task_name'])\n",
    "\n",
    "optimizer = torch.optim.SGD(lr=train_config['lr'],\n",
    "                            params=filter(lambda p: p.requires_grad,\n",
    "                                            yolo_model.parameters()),\n",
    "                            weight_decay=5E-4,\n",
    "                            momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QyzIS6gY-CK4"
   },
   "outputs": [],
   "source": [
    "scheduler = MultiStepLR(optimizer, milestones=train_config['lr_steps'], gamma=0.5)\n",
    "\n",
    "from CV.object_detection.yolo_version1.loss import YOLOV1Loss\n",
    "criterion = YOLOV1Loss()\n",
    "\n",
    "acc_steps = train_config['acc_steps']\n",
    "num_epochs = train_config['num_epochs']\n",
    "steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qhzGFRTb9Wqy"
   },
   "outputs": [],
   "source": [
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    losses = []\n",
    "    optimizer.zero_grad()\n",
    "    for idx, (ims, targets, _) in enumerate(tqdm(train_dataset)):\n",
    "        yolo_targets = torch.cat([\n",
    "            target['yolo_targets'].unsqueeze(0).float().to(device)\n",
    "            for target in targets], dim=0)\n",
    "        im = torch.cat([im.unsqueeze(0).float().to(device) for im in ims], dim=0)\n",
    "        yolo_preds = yolo_model(im)\n",
    "        loss = criterion(yolo_preds, yolo_targets, use_sigmoid=model_config['use_sigmoid'])\n",
    "        loss = loss / acc_steps\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        if (idx + 1) % acc_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        if steps % train_config['log_steps'] == 0:\n",
    "            print('Loss : {:.4f}'.format(np.mean(losses)))\n",
    "        if torch.isnan(loss):\n",
    "            print('Loss is becoming nan. Exiting')\n",
    "            exit(0)\n",
    "        steps += 1\n",
    "    print('Finished epoch {}'.format(epoch_idx+1))\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    scheduler.step()\n",
    "    torch.save(yolo_model.state_dict(), os.path.join(train_config['task_name'],\n",
    "                                                        train_config['ckpt_name']))\n",
    "print('Done Training...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39jLK65H9XA5"
   },
   "source": [
    "# **Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oGSmE1FV9YxX"
   },
   "outputs": [],
   "source": [
    "from CV.object_detection.yolo_version1.infer import infer, evaluate_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pR7CzaxFFnEJ"
   },
   "outputs": [],
   "source": [
    "infer(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OlYjShK0GBBx"
   },
   "outputs": [],
   "source": [
    "evaluate_map(args)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "TqbKXAjk9Uwk",
    "nK5pM4US9hCi",
    "2dzSSurx9WIT",
    "39jLK65H9XA5"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
