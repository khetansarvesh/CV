{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["x8Y95WxnjWhT","4VqLfzASqjbQ","90uqayO0ql8d","LRUvSzmooi_T","2rweJmud-82x","M2D_gvEG-keS","4Qazk1M0-oAc"],"authorship_tag":"ABX9TyOBgZVFxZJ5uVWFJjY5hzTN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# !pip install gymnasium\n","import gymnasium as gym\n","print(gym.__version__)\n","import tensorflow as tf\n","import numpy as np"],"metadata":{"id":"K_B97AwZsGM6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Creating Environment Using [GYM](https://github.com/Farama-Foundation/Gymnasium) - CartPole Game\n"],"metadata":{"id":"x8Y95WxnjWhT"}},{"cell_type":"code","source":["# environment is defined by 4 variables here [horizontal pos, horizontal velocity, angle of pole, angular velocity]\n","env = gym.make('CartPole-v1')\n","\n","# Reset the environment to default beginning\n","observation = env.reset()\n","print(observation)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3mKfwTM5jmU7","executionInfo":{"status":"ok","timestamp":1698049806941,"user_tz":-330,"elapsed":7,"user":{"displayName":"Sarvesh Khetan","userId":"05805229168588767785"}},"outputId":"fe1702b7-d79f-4b20-d0f1-2849e37859d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(array([-0.02135036, -0.04951756, -0.03659555,  0.03179171], dtype=float32), {})\n"]}]},{"cell_type":"markdown","source":["# Performing some random action"],"metadata":{"id":"4VqLfzASqjbQ"}},{"cell_type":"code","source":["# Performing 1 Random Action and seeing the change in the environment via observation value change\n","action = env.action_space.sample()\n","observation, reward, terminated, truncated, info = env.step(action)\n","print(observation, reward, terminated, truncated , info)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A11bs8EpmZ7x","executionInfo":{"status":"ok","timestamp":1698049083519,"user_tz":-330,"elapsed":609,"user":{"displayName":"Sarvesh Khetan","userId":"05805229168588767785"}},"outputId":"86b44bd7-1629-4252-d791-516913f567ac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[ 0.00472824  0.3598044  -0.06903811 -0.6166674 ] 1.0 False False {}\n"]}]},{"cell_type":"code","source":["# Performing 1000 random actions and visulizing each action using rendering in gym\n","for t in range(1000):\n","    env.render() # this wont work in google colab, you will have to run this on local as a python file and not a notebook file\n","\n","    action = env.action_space.sample()\n","    observation, reward, terminated, truncated, info = env.step(action)\n","    # print(observation, reward, terminated, truncated , info)"],"metadata":{"id":"wZwDwGZ3kuHa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Perform some specific action using manually defined policy"],"metadata":{"id":"90uqayO0ql8d"}},{"cell_type":"code","source":["for t in range(1000):\n","    env.render()\n","    pole_ang = observation[2]\n","\n","    if pole_ang > 0: # means pole is falling to right\n","        action = 1 # hence move the cart to right to balance the pole\n","    else: # means pole is falling to left\n","        action = 0 # hence move the cart to left to balance the pole\n","\n","    # Perform Action\n","    observation, reward, terminated, truncated, info = env.step(action)\n","    # print(observation, reward, terminated, truncated , info)"],"metadata":{"id":"Jw0rDY7shIk8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698049796213,"user_tz":-330,"elapsed":10,"user":{"displayName":"Sarvesh Khetan","userId":"05805229168588767785"}},"outputId":"ccd9f2cf-dc62-49b9-9275-62bb51ee6676"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gymnasium/envs/classic_control/cartpole.py:180: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n","  logger.warn(\n"]}]},{"cell_type":"markdown","source":["# Learning policy using FFNN (using only the current state) and hence perform action"],"metadata":{"id":"LRUvSzmooi_T"}},{"cell_type":"code","source":["num_inputs = 4 #Observation Space has 4 inputs\n","num_hidden = 4 # only one hidden layer\n","num_outputs = 1 #Outputs the probability it should go left\n","learning_rate = 0.01\n","initializer = tf.contrib.layers.variance_scaling_initializer()"],"metadata":{"id":"bqCkZolyswwU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# defining the FFNN architecture\n","X = tf.placeholder(tf.float32, shape=[None,num_inputs])\n","hidden_layer_one = tf.layers.dense(X,num_hidden,activation=tf.nn.relu,kernel_initializer=initializer) # this is actually input layer and not hidden layer\n","hidden_layer_two = tf.layers.dense(hidden_layer_one,num_hidden,activation=tf.nn.relu,kernel_initializer=initializer) # hidden layer 1\n","output_layer = tf.layers.dense(hidden_layer_one,num_outputs,activation=tf.nn.sigmoid,kernel_initializer=initializer) # Output Layer - Probability to go left\n","probabilties = tf.concat(axis=1, values=[output_layer, 1 - output_layer]) # [ Prob to go left , Prob to go right]\n","action = tf.multinomial(probabilties, num_samples=1) # Sample 1 randomly based on probabilities\n","init = tf.global_variables_initializer()"],"metadata":{"id":"6q4Elhfss1Fk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# trianing the above architecture\n","saver = tf.train.Saver()\n","env = gym.make(\"CartPole-v1\")\n","\n","with tf.Session() as sess:\n","    init.run()\n","\n","    # we will run the game for 50 different times\n","    for i_episode in range(50):\n","        obs = env.reset() # new game start is defined by resetting the environment\n","\n","        # for each gameplay predicting action for 500 times i.e. playing the game by taking action 500 times\n","        for step in range(500):\n","            # env.render()\n","            action_val = action.eval(feed_dict={X: obs.reshape(1, num_inputs)}) #training using the above network and by inputing the observations\n","            observation, reward, terminated, truncated, info = env.step(action_val[0][0]) #performing the action and the getting the new observation\n","\n","env.close()"],"metadata":{"id":"n77IHf8_hO3w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Learning policy using FFNN (using all the previous states) and hence perform action"],"metadata":{"id":"2rweJmud-82x"}},{"cell_type":"markdown","source":["## Training the network"],"metadata":{"id":"M2D_gvEG-keS"}},{"cell_type":"code","source":["X = tf.placeholder(tf.float32, shape=[None, num_inputs])\n","\n","hidden_layer = tf.layers.dense(X, num_hidden, activation=tf.nn.elu, kernel_initializer=initializer) # this is actually input layer\n","logits = tf.layers.dense(hidden_layer, num_outputs) # output layer\n","outputs = tf.nn.sigmoid(logits)  # probability of action 0 (left)\n","\n","probabilties = tf.concat(axis=1, values=[outputs, 1 - outputs])\n","action = tf.multinomial( probabilties, num_samples=1)\n","y = 1. - tf.to_float(action) # Convert from Tensor to number for network training\n","\n","\n","cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n","optimizer = tf.train.AdamOptimizer(learning_rate)\n","gradients_and_variables = optimizer.compute_gradients(cross_entropy)\n","\n","gradients = []\n","gradient_placeholders = []\n","grads_and_vars_feed = []\n","\n","for gradient, variable in gradients_and_variables:\n","    gradients.append(gradient)\n","    gradient_placeholder = tf.placeholder(tf.float32, shape=gradient.get_shape())\n","    gradient_placeholders.append(gradient_placeholder)\n","    grads_and_vars_feed.append((gradient_placeholder, variable))\n","\n","\n","training_op = optimizer.apply_gradients(grads_and_vars_feed)\n","\n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()\n","\n","######################################\n","#### REWARD FUNCTIONs ################\n","####################################\n","# CHECK OUT: https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724\n","\n","def helper_discount_rewards(rewards, discount_rate):\n","    '''\n","    Takes in rewards and applies discount rate\n","    '''\n","    discounted_rewards = np.zeros(len(rewards))\n","    cumulative_rewards = 0\n","    for step in reversed(range(len(rewards))):\n","        cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate\n","        discounted_rewards[step] = cumulative_rewards\n","    return discounted_rewards\n","\n","def discount_and_normalize_rewards(all_rewards, discount_rate):\n","    '''\n","    Takes in all rewards, applies helper_discount function and then normalizes\n","    using mean and std.\n","    '''\n","    all_discounted_rewards = []\n","    for rewards in all_rewards:\n","        all_discounted_rewards.append(helper_discount_rewards(rewards,discount_rate))\n","\n","    flat_rewards = np.concatenate(all_discounted_rewards)\n","    reward_mean = flat_rewards.mean()\n","    reward_std = flat_rewards.std()\n","    return [(discounted_rewards - reward_mean)/reward_std for discounted_rewards in all_discounted_rewards]\n","\n","########################################\n","#### TRAINING SESSION #################\n","######################################\n","\n","env = gym.make(\"CartPole-v0\")\n","with tf.Session() as sess:\n","    sess.run(init)\n","\n","\n","    for iteration in range(250):\n","        all_rewards = []\n","        all_gradients = []\n","\n","        # Play 10 game rounds\n","        for game in range(10):\n","            current_rewards = []\n","            current_gradients = []\n","            observations = env.reset()\n","\n","            # Only allow 1000 amount of steps in game\n","            for step in range(1000):\n","                action_val, gradients_val = sess.run([action, gradients], feed_dict={X: observations.reshape(1, num_inputs)}) # Get Actions and Gradients\n","                observations, reward, done, info = env.step(action_val[0][0]) # Perform Action\n","                current_rewards.append(reward) # Get Current Rewards and Gradients\n","                current_gradients.append(gradients_val)\n","                if done: # means game ended\n","                    break\n","\n","            # Append to list of all rewards\n","            all_rewards.append(current_rewards)\n","            all_gradients.append(current_gradients)\n","\n","        all_rewards = discount_and_normalize_rewards(all_rewards,0.95)\n","        feed_dict = {}\n","\n","\n","        for var_index, gradient_placeholder in enumerate(gradient_placeholders):\n","            mean_gradients = np.mean([reward * all_gradients[game_index][step][var_index]\n","                                      for game_index, rewards in enumerate(all_rewards)\n","                                          for step, reward in enumerate(rewards)], axis=0)\n","            feed_dict[gradient_placeholder] = mean_gradients\n","\n","        sess.run(training_op, feed_dict=feed_dict)\n","\n","    print('SAVING GRAPH AND SESSION')\n","    meta_graph_def = tf.train.export_meta_graph(filename='/models/my-650-step-model.meta')\n","    saver.save(sess, '/models/my-650-step-model')"],"metadata":{"id":"OizC8n9ohUms"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Predicting using trained network"],"metadata":{"id":"4Qazk1M0-oAc"}},{"cell_type":"code","source":["#############################################\n","### RUN TRAINED MODEL ON ENVIRONMENT #######\n","###########################################\n","\n","env = gym.make('CartPole-v0')\n","\n","observations = env.reset()\n","with tf.Session() as sess:\n","    new_saver = tf.train.import_meta_graph('/models/my-650-step-model.meta')\n","    new_saver.restore(sess,'/models/my-650-step-model')\n","\n","    for x in range(500):\n","        env.render()\n","        action_val, gradients_val = sess.run([action, gradients], feed_dict={X: observations.reshape(1, num_inputs)})\n","        observations, reward, done, info = env.step(action_val[0][0])"],"metadata":{"id":"Wwq8HFiV-S-l"},"execution_count":null,"outputs":[]}]}